{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (StandardScaler, OneHotEncoder, FunctionTransformer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlations\n",
    "def plotCoorelations(df):\n",
    "    # remove non_numeric features  \n",
    "    corr = df.corr()\n",
    "    corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_IsolationForest(home_data):\n",
    "    X_numeric = home_data.select_dtypes(exclude=object) \n",
    "    imp_mean = SimpleImputer()\n",
    "    X_numeric_t = imp_mean.fit_transform(X_numeric)\n",
    "    clf = IsolationForest( behaviour = 'new', contamination= 0.02)\n",
    "    preds = clf.fit_predict(X_numeric_t)\n",
    "    outliers = np.where(preds == -1)\n",
    "\n",
    "    # drop outliers from home data\n",
    "    home_data_out = home_data.drop(labels = outliers[0], inplace=False, errors = \"ignore\")\n",
    "    #home_data_out = home_data[(preds == -1).all(axis=1)]\n",
    "    print(\"number of outliers = {0}\".format(len(outliers[0])))\n",
    "    return home_data_out, outliers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_ZScore(home_data, zValue = 3):\n",
    "    X_numeric = home_data.select_dtypes(exclude=object) \n",
    "    \n",
    "    z = np.abs(stats.zscore(X_numeric))\n",
    "    outliers = np.where(z > zValue)\n",
    "\n",
    "    # drop outliers from home data\n",
    "    home_data_out = home_data.drop(labels = outliers[0], inplace=False, errors = \"ignore\")\n",
    "    #home_data_out = home_data[(preds == -1).all(axis=1)]\n",
    "    print(\"number of outliers = {0}\".format(len(outliers[0])))\n",
    "    return home_data_out, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 80)\n",
      "number of outliers = 30\n"
     ]
    }
   ],
   "source": [
    "# load housing data\n",
    "iowa_file_path = '../data/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "Y = home_data[\"SalePrice\"]\n",
    "X = home_data.drop(columns = [\"Id\"])\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=1)\n",
    "print(X_train.shape)\n",
    "\n",
    "# look at coorelations\n",
    "#plotCoorelations(X_train)\n",
    "\n",
    "# perform mean normalization and feature scaling on the data\n",
    "\n",
    "home_data, dummy = outlierDetect_IsolationForest(home_data.copy())\n",
    "#home_data, dummy = outlierDetect_ZScore(X_train.copy(), 6)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train our classifier with the following features:\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = X_train.select_dtypes(exclude=object) \n",
    "num_features_names = numeric_features.columns\n",
    "print(numeric_features_names)\n",
    "\n",
    "# features that need a log transformation\n",
    "log_features_names = [\"LotFrontage\", \"LotArea\", \"1stFlrSF\", \"GrLivArea\", \"OpenPorchSF\"]\n",
    "log_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('logscaler', StandardScaler())])\n",
    "\n",
    "#kbinDiscretizer features\n",
    "\n",
    "\n",
    "#numeric features that require a normal transformation\n",
    "#numeric_features_names = [ x for x in num_features_names if x not in log_features_names]\n",
    "is_numeric = np.isin(numeric_features_names, log_features_names, invert = False)\n",
    "print(is_numeric)\n",
    "#numeric_features_names = numeric_features_names[is_numeric]\n",
    "\n",
    "print(len(numeric_features_names))\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features_names = X.select_dtypes(include=object).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log', log_transformer, log_features_names),\n",
    "        ('num', numeric_transformer, numeric_features_names),\n",
    "        ('cat', categorical_transformer, categorical_features_names)\n",
    "        \n",
    "    ])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestRegressor(random_state=1, n_estimators = 500, criterion=\"mae\"))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting and prediction\n",
    "clf.fit_transform(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "rf_val_mae = mean_absolute_error(y_test, y_predict)\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n",
    "\n",
    "#train the best model on the full dat\n",
    "clf.fit(X,Y)\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "X_test2 = test_data.drop(columns = [\"Id\"])\n",
    "\n",
    "#make predictions which we will submit. \n",
    "y_pred2 = clf.predict(X_test2)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred2})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
