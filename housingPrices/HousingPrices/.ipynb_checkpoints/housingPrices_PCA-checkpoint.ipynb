{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (StandardScaler, OneHotEncoder, FunctionTransformer, KBinsDiscretizer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KitchenQual', 'GarageCond', 'GarageQual', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'LotConfig', 'Utilities', 'LandSlope', 'LotShape', 'GarageType', 'BldgType', 'CentralAir', 'MSZoning', 'LandContour', 'PavedDrive']\n"
     ]
    }
   ],
   "source": [
    "# constants and helper methods\n",
    "\n",
    "CONDITIONS_DICT = {\"NA\": 0, \"NaN\": 0, \"nan\": 0, \"Po\": 2, \"Fa\": 3, \"TA\": 4, \"Gd\":6, \"Ex\": 10}\n",
    "\n",
    "# constants\n",
    "CATEGORY_LABELS = {\"KitchenQual\":       CONDITIONS_DICT,\n",
    "                    \"GarageCond\":       CONDITIONS_DICT,\n",
    "                    \"GarageQual\":       CONDITIONS_DICT,\n",
    "                    \"ExterQual\":        CONDITIONS_DICT,\n",
    "                    \"ExterCond\":        CONDITIONS_DICT,\n",
    "                    \"BsmtQual\":         CONDITIONS_DICT,\n",
    "                    \"BsmtCond\":         CONDITIONS_DICT,\n",
    "                    \"FireplaceQu\" :     CONDITIONS_DICT,\n",
    "                    \"HeatingQC\" :       CONDITIONS_DICT,\n",
    "                    \"LotConfig\":     {\"Inside\": 0, \"Corner\": 6, \"CulDSac\": 10, \"FR2\": 3, \"FR3\":4},\n",
    "                    \"Utilities\":     {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "                    \"LandSlope\":     {\"Gtl\": 10, \"Mod\": 4, \"Sev\": 1},\n",
    "                    \"LotShape\":     {\"Reg\": 10, \"IR1\": 5, \"IR2\": 3, \"IR3\": 1},\n",
    "                    \"GarageType\":     {\"NA\": 0, \"nan\": 0, \"Basment\": 4,  \"Detchd\": 1, \"CarPort\": 3, \"BuiltIn\": 5, \"Attchd\": 7, \"2Types\": 12},\n",
    "                    \"BldgType\":     {\"TwnhsI\": 1, \"Twnhs\": 2, \"TwnhsE\": 3, \"Duplex\": 5,  \"2fmCon\": 7, \"1Fam\": 12},\n",
    "                    \"CentralAir\":     {\"N\": 1, \"Y\": 10},\n",
    "                    \"Electrical\":     {\"Mix\": 1, \"FuseP\": 3, \"FuseF\": 5,  \"FuseA\": 7, \"SBrkr\": 12},\n",
    "                    \"MSZoning\":     {\"RL\": 100, \"RM\": 60, \"C (all)\": 20, \"FV\": 30, \"RH\": 30},\n",
    "                    \"LandContour\":     {\"Lvl\": 100, \"Low\": 15, \"Bnk\": 25, \"HLS\": 5},\n",
    "                    \"Fence\":     {\"NA\": 0, \"MnPrv\": 25, \"MnWw\": 15, \"GdWo\": 40, 'GdPrv': 100},\n",
    "                    \"Functional\":     {\"Typ\": 100, \"Min1\": 70, \"Min2\": 50, \"Mod\": 40, \"Maj1\": 25, \"Maj2\": 20, \"Min2\": 10, \"Sev\": 5, \"Sal\": 1},\n",
    "                    \"MiscFeature\":     {\"NA\": 0, \"Shed\": 30, \"Gar2\": 40, \"Othr\": 25, \"TenC\": 100},\n",
    "                    \"PavedDrive\":     {\"Y\": 100, \"P\": 30, \"N\": 0},\n",
    "                    }\n",
    "\n",
    "CAT_COLS_TO_IGNORE = [\"Functional\",\n",
    "                        \"MiscFeature\",\n",
    "                        \"Electrical\",\n",
    "                        \"Fence\",\n",
    "                        \"FireplaceQu\",\n",
    "                        \"HeatingQC\"           \n",
    "                    ]\n",
    "\n",
    "CAT_COLS = [ x for x in CATEGORY_LABELS.keys() if x not in CAT_COLS_TO_IGNORE]\n",
    "print(CAT_COLS)\n",
    "\n",
    "# plot correlations\n",
    "def plotCoorelations(df):\n",
    "    # remove non_numeric features  \n",
    "    corr = df.corr()\n",
    "    corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierRemoval_IsolationForest(X, y, outlierFraction = 0.02):\n",
    "    clf = IsolationForest( behaviour = 'new', contamination = outlierFraction)\n",
    "    preds = clf.fit_predict(X)\n",
    "    outliers = np.where(preds == -1)\n",
    "    return dropOutliers(X, y, outliers)\n",
    "\n",
    "def dropOutliers(X, y, outliers):\n",
    "    print(\"number of outliers = {0}\".format(len(outliers[0])))\n",
    "    # drop outliers\n",
    "    X_clean = np.delete(X, outliers[0], axis = 0)\n",
    "    y_clean = np.delete(y.values, outliers[0])\n",
    "    return X_clean, y_clean, outliers[0] \n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierRemoval_ZScore(X, y, zValue = 3, bypass=False):\n",
    "    if bypass:\n",
    "        return X, y, []\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    outliers = np.where(z > zValue)\n",
    "    return dropOutliers(X, y, outliers)\n",
    "\n",
    "def getTransformedColumnNames(ct):\n",
    "    for item in ct.named_transformers_:\n",
    "       pipeline = ct.named_transformers_[item]\n",
    "       for step in pipeline.named_steps:\n",
    "           t1 = pipeline.named_steps[step]\n",
    "           print(t1.get_feature_names())\n",
    "            \n",
    "  \n",
    "def displayScoresExp1p(scores):\n",
    "    print(\"Scores:\", np.expm1(scores))\n",
    "    print(\"Mean:\", np.expm1(scores.mean()))\n",
    "    print(\"standard deviation:\", np.expm1(scores.std()))\n",
    "    \n",
    "def displayScores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"standard deviation:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "iowa_file_path = '../data/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "Y = home_data[\"SalePrice\"]\n",
    "X = home_data.drop(columns = [\"Id\", \"SalePrice\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature scaling for numerical features\n",
    "numeric_features = X.select_dtypes(exclude=object) \n",
    "num_features_names = numeric_features.columns\n",
    "\n",
    "# features that need a log transformation\n",
    "log_features_names = [\"LotFrontage\", \"LotArea\", \"1stFlrSF\", \"GrLivArea\", \"OpenPorchSF\"]\n",
    "\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('logscaler', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#numeric features that require a normal transformation\n",
    "numeric_features_names = [x for x in num_features_names if x not in log_features_names]\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "transformers=[\n",
    "        ('log', log_pipeline, log_features_names),\n",
    "        ('num', numeric_pipeline, numeric_features_names),  \n",
    "    ]\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "ct = ColumnTransformer(transformers=transformers, sparse_threshold = 0)\n",
    "Xt = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of outliers = 36\n"
     ]
    }
   ],
   "source": [
    "# remove outliers from numeric data\n",
    "Xt, Y, outliers = outlierRemoval_ZScore(Xt, Y, 8, bypass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xt shape:  (1427, 36)\n",
      "X_pca_t shape:  (1427, 31)\n"
     ]
    }
   ],
   "source": [
    "# use PCA to fit and transform the data using a 0.95 variance\n",
    "pca = PCA(0.99)\n",
    "X_pca_t = pca.fit_transform(Xt)\n",
    "print(\"Xt shape: \", Xt.shape)\n",
    "print(\"X_pca_t shape: \", X_pca_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_names = X.select_dtypes(include=object).columns\n",
    "#categorical_features_names = CAT_COLS\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "X_cat_t = cat_pipeline.fit_transform(X[categorical_features_names])\n",
    "\n",
    "# remove corresponding outlier rows from the categorical data\n",
    "X_cat_t2 = np.delete(X_cat_t.todense(), outliers, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated Xt shape:  (1427, 299)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the numerical and one hot encoded categorical data\n",
    "Xt = np.concatenate((X_pca_t, X_cat_t2 ), axis = 1)\n",
    "print(\"concatenated Xt shape: \", Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random forest regressor \n",
    "\n",
    "clf = RandomForestRegressor(random_state=1, n_estimators = 100, criterion=\"mae\", n_jobs=-1)\n",
    "print(Xt.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(clf, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayScores(-scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.2s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 0.41911959648132324 seconds\n",
      "Scores: [16535.30262927 16230.0643879  18073.45307663 19802.65871948\n",
      " 19264.47623081 17784.61381912 16858.0509664  15379.02919795\n",
      " 18532.69324927 14167.95265616]\n",
      "Mean: 17262.82949329868\n",
      "standard deviation: 1669.9088760063983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha = 0.1, solver=\"auto\")\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(ridge_reg, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    3.0s remaining:    2.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 4.740268230438232 seconds\n",
      "Scores: [16718.07698855 16459.41663437 18243.65270362 19851.94331315\n",
      " 19535.52586534 18200.39398637 16802.95985102 15550.72750226\n",
      " 18529.8058102  14084.99494838]\n",
      "Mean: 17397.74976032493\n",
      "standard deviation: 1712.2069952847648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(lasso_reg, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 0.7573914527893066 seconds\n",
      "Scores: [15723.11342325 15413.91786181 16277.94231296 18477.75985012\n",
      " 17725.58820143 14219.5079973  14923.36250838 13817.21215037\n",
      " 17115.50499708 14805.20570642]\n",
      "Mean: 15849.91150091082\n",
      "standard deviation: 1454.4703839414574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(elastic_net, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=ElasticNet(alpha=0.2, copy_X=True, fit_intercept=True,\n",
       "                                  l1_ratio=0.9, max_iter=1000, normalize=False,\n",
       "                                  positive=False, precompute=False,\n",
       "                                  random_state=None, selection='cyclic',\n",
       "                                  tol=0.0001, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'alpha': [0.2, 0.5, 0.7, 1.0],\n",
       "                          'l1_ratio': [0.1, 0.4, 0.5, 0.7, 0.8, 0.9]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic net seems to give the best scores. use a gridsearchCV to find the best elasticNet parameters\n",
    "param_grid = [\n",
    "    {'alpha' : [0.2, 0.5, 0.7,  1.0], 'l1_ratio': [ 0.1,0.4,  0.5, 0.7, 0.8, 0.9]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 10, cv = 10)\n",
    "grid_search.fit(Xt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.2, 'l1_ratio': 0.9}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-6b803ce87aaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#make predictions which we will submit.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melastic_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#The lines below shows how to save predictions in format used for competition scoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rajes\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rajes\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    793\u001b[0m                                    dense_output=True) + self.intercept_\n\u001b[0;32m    794\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rajes\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coef_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m    206\u001b[0m                                dense_output=True) + self.intercept_\n",
      "\u001b[1;32mc:\\users\\rajes\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Normal'"
     ]
    }
   ],
   "source": [
    "# use the grid serach results to create the final predictor for the test\n",
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "elastic_net.fit(Xt,Y)\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "X_test = test_data.drop(columns = [\"Id\"])\n",
    "\n",
    "# numerical columns\n",
    "X_num_test = ct.fit_transform(X_test)\n",
    "# PCA on the numerical data\n",
    "X_pca_test = pca.transform(X_num_test)\n",
    "# categorical colums that are OHE\n",
    "X_cat_test = cat_pipeline.transform(X_test[categorical_features_names])\n",
    "\n",
    "\n",
    "#make predictions which we will submit. \n",
    "y_pred = elastic_net.predict(X_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
