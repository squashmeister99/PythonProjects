{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (StandardScaler, OneHotEncoder, FunctionTransformer, KBinsDiscretizer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KitchenQual', 'GarageCond', 'GarageQual', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'LotConfig', 'Utilities', 'LandSlope', 'LotShape', 'GarageType', 'BldgType', 'CentralAir', 'MSZoning', 'LandContour', 'PavedDrive']\n"
     ]
    }
   ],
   "source": [
    "# constants and helper methods\n",
    "\n",
    "CONDITIONS_DICT = {\"NA\": 0, \"NaN\": 0, \"nan\": 0, \"Po\": 2, \"Fa\": 3, \"TA\": 4, \"Gd\":6, \"Ex\": 10}\n",
    "\n",
    "# constants\n",
    "CATEGORY_LABELS = {\"KitchenQual\":       CONDITIONS_DICT,\n",
    "                    \"GarageCond\":       CONDITIONS_DICT,\n",
    "                    \"GarageQual\":       CONDITIONS_DICT,\n",
    "                    \"ExterQual\":        CONDITIONS_DICT,\n",
    "                    \"ExterCond\":        CONDITIONS_DICT,\n",
    "                    \"BsmtQual\":         CONDITIONS_DICT,\n",
    "                    \"BsmtCond\":         CONDITIONS_DICT,\n",
    "                    \"FireplaceQu\" :     CONDITIONS_DICT,\n",
    "                    \"HeatingQC\" :       CONDITIONS_DICT,\n",
    "                    \"LotConfig\":     {\"Inside\": 0, \"Corner\": 6, \"CulDSac\": 10, \"FR2\": 3, \"FR3\":4},\n",
    "                    \"Utilities\":     {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "                    \"LandSlope\":     {\"Gtl\": 10, \"Mod\": 4, \"Sev\": 1},\n",
    "                    \"LotShape\":     {\"Reg\": 10, \"IR1\": 5, \"IR2\": 3, \"IR3\": 1},\n",
    "                    \"GarageType\":     {\"NA\": 0, \"nan\": 0, \"Basment\": 4,  \"Detchd\": 1, \"CarPort\": 3, \"BuiltIn\": 5, \"Attchd\": 7, \"2Types\": 12},\n",
    "                    \"BldgType\":     {\"TwnhsI\": 1, \"Twnhs\": 2, \"TwnhsE\": 3, \"Duplex\": 5,  \"2fmCon\": 7, \"1Fam\": 12},\n",
    "                    \"CentralAir\":     {\"N\": 1, \"Y\": 10},\n",
    "                    \"Electrical\":     {\"Mix\": 1, \"FuseP\": 3, \"FuseF\": 5,  \"FuseA\": 7, \"SBrkr\": 12},\n",
    "                    \"MSZoning\":     {\"RL\": 100, \"RM\": 60, \"C (all)\": 20, \"FV\": 30, \"RH\": 30},\n",
    "                    \"LandContour\":     {\"Lvl\": 100, \"Low\": 15, \"Bnk\": 25, \"HLS\": 5},\n",
    "                    \"Fence\":     {\"NA\": 0, \"MnPrv\": 25, \"MnWw\": 15, \"GdWo\": 40, 'GdPrv': 100},\n",
    "                    \"Functional\":     {\"Typ\": 100, \"Min1\": 70, \"Min2\": 50, \"Mod\": 40, \"Maj1\": 25, \"Maj2\": 20, \"Min2\": 10, \"Sev\": 5, \"Sal\": 1},\n",
    "                    \"MiscFeature\":     {\"NA\": 0, \"Shed\": 30, \"Gar2\": 40, \"Othr\": 25, \"TenC\": 100},\n",
    "                    \"PavedDrive\":     {\"Y\": 100, \"P\": 30, \"N\": 0},\n",
    "                    }\n",
    "\n",
    "CAT_COLS_TO_IGNORE = [\"Functional\",\n",
    "                        \"MiscFeature\",\n",
    "                        \"Electrical\",\n",
    "                        \"Fence\",\n",
    "                        \"FireplaceQu\",\n",
    "                        \"HeatingQC\"           \n",
    "                    ]\n",
    "\n",
    "CAT_COLS = [ x for x in CATEGORY_LABELS.keys() if x not in CAT_COLS_TO_IGNORE]\n",
    "print(CAT_COLS)\n",
    "\n",
    "# plot correlations\n",
    "def plotCoorelations(df):\n",
    "    # remove non_numeric features  \n",
    "    corr = df.corr()\n",
    "    corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierRemoval_IsolationForest(X, y, outlierFraction = 0.02):\n",
    "    clf = IsolationForest( behaviour = 'new', contamination = outlierFraction)\n",
    "    preds = clf.fit_predict(X)\n",
    "    outliers = np.where(preds == -1)\n",
    "    return dropOutliers(X, y, outliers)\n",
    "\n",
    "def dropOutliers(X, y, outliers):\n",
    "    print(\"number of outliers = {0}\".format(len(outliers[0])))\n",
    "    # drop outliers\n",
    "    X_clean = np.delete(X, outliers[0], axis = 0)\n",
    "    y_clean = np.delete(y.values, outliers[0])\n",
    "    return X_clean, y_clean, outliers[0] \n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierRemoval_ZScore(X, y, zValue = 3, bypass=False):\n",
    "    if bypass:\n",
    "        return X, y, []\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    outliers = np.where(z > zValue)\n",
    "    return dropOutliers(X, y, outliers)\n",
    "\n",
    "def getTransformedColumnNames(ct):\n",
    "    for item in ct.named_transformers_:\n",
    "       pipeline = ct.named_transformers_[item]\n",
    "       for step in pipeline.named_steps:\n",
    "           t1 = pipeline.named_steps[step]\n",
    "           print(t1.get_feature_names())\n",
    "            \n",
    "  \n",
    "def displayScoresExp1p(scores):\n",
    "    print(\"Scores:\", np.expm1(scores))\n",
    "    print(\"Mean:\", np.expm1(scores.mean()))\n",
    "    print(\"standard deviation:\", np.expm1(scores.std()))\n",
    "    \n",
    "def displayScores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"standard deviation:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "iowa_file_path = '../data/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers\n",
    "home_data = home_data.drop(home_data['LotFrontage']\n",
    "                                     [home_data['LotFrontage']>200].index)\n",
    "home_data = home_data.drop(home_data['LotArea']\n",
    "                                     [home_data['LotArea']>100000].index)\n",
    "home_data = home_data.drop(home_data['BsmtFinSF1']\n",
    "                                     [home_data['BsmtFinSF1']>4000].index)\n",
    "home_data = home_data.drop(home_data['TotalBsmtSF']\n",
    "                                     [home_data['TotalBsmtSF']>6000].index)\n",
    "home_data = home_data.drop(home_data['1stFlrSF']\n",
    "                                     [home_data['1stFlrSF']>4000].index)\n",
    "home_data = home_data.drop(home_data.LowQualFinSF\n",
    "                                     [home_data['LowQualFinSF']>550].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = home_data[\"SalePrice\"]\n",
    "X = home_data.drop(columns = [\"Id\", \"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature scaling for numerical features\n",
    "numeric_features = X.select_dtypes(exclude=object) \n",
    "num_features_names = numeric_features.columns\n",
    "\n",
    "# features that need a log transformation\n",
    "log_features_names = [\"LotFrontage\", \"LotArea\", \"1stFlrSF\", \"GrLivArea\", \"OpenPorchSF\"]\n",
    "\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('logscaler', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#numeric features that require a normal transformation\n",
    "numeric_features_names = [x for x in num_features_names if x not in log_features_names]\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "transformers=[\n",
    "        ('log', log_pipeline, log_features_names),\n",
    "        ('num', numeric_pipeline, numeric_features_names),  \n",
    "    ]\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "ct = ColumnTransformer(transformers=transformers, sparse_threshold = 0)\n",
    "Xt = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from numeric data\n",
    "Xt, Y, outliers = outlierRemoval_ZScore(Xt, Y, 8, bypass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xt shape:  (1453, 36)\n",
      "X_pca_t shape:  (1453, 32)\n"
     ]
    }
   ],
   "source": [
    "# use PCA to fit and transform the data using a 0.95 variance\n",
    "pca = PCA(0.99)\n",
    "X_pca_t = pca.fit_transform(Xt)\n",
    "print(\"Xt shape: \", Xt.shape)\n",
    "print(\"X_pca_t shape: \", X_pca_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_names = X.select_dtypes(include=object).columns\n",
    "#categorical_features_names = CAT_COLS\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "X_cat_t = cat_pipeline.fit_transform(X[categorical_features_names])\n",
    "\n",
    "# remove corresponding outlier rows from the categorical data\n",
    "X_cat_t2 = np.delete(X_cat_t.todense(), outliers, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated Xt shape:  (1453, 299)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the numerical and one hot encoded categorical data\n",
    "Xt = np.concatenate((X_pca_t, X_cat_t2 ), axis = 1)\n",
    "print(\"concatenated Xt shape: \", Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random forest regressor \n",
    "\n",
    "clf = RandomForestRegressor(random_state=1, n_estimators = 100, criterion=\"mae\", n_jobs=-1)\n",
    "print(Xt.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(clf, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayScores(-scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 2.2111661434173584 seconds\n",
      "Scores: [16465.16959213 18225.30628047 18255.28813667 19827.58313817\n",
      " 20477.93474478 19864.05317795 16561.17837685 16306.64426933\n",
      " 20864.92408848 14854.21868569]\n",
      "Mean: 18170.230049052312\n",
      "standard deviation: 1953.9233101178902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    2.0s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha = 0.1, solver=\"auto\")\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(ridge_reg, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    2.7s remaining:    1.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 3.8688018321990967 seconds\n",
      "Scores: [16762.57302271 18472.89073683 18530.10763561 19875.2405406\n",
      " 20586.2274657  20323.22356883 16506.48917108 16346.20203232\n",
      " 20964.05727597 18958.08199806]\n",
      "Mean: 18732.5093447715\n",
      "standard deviation: 1641.3961185873336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(lasso_reg, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 0.652238130569458 seconds\n",
      "Scores: [16172.94900749 16173.17659262 16675.81966436 18876.69768604\n",
      " 18075.2008204  15207.34116078 15007.51638548 14521.20329742\n",
      " 18972.85280259 15387.28135757]\n",
      "Mean: 16507.003877475196\n",
      "standard deviation: 1534.32075469751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(elastic_net, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1672s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:    8.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=ElasticNet(alpha=0.2, copy_X=True, fit_intercept=True,\n",
       "                                  l1_ratio=0.9, max_iter=1000, normalize=False,\n",
       "                                  positive=False, precompute=False,\n",
       "                                  random_state=None, selection='cyclic',\n",
       "                                  tol=0.0001, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'alpha': [0.2, 0.5, 0.7, 1.0],\n",
       "                          'l1_ratio': [0.1, 0.4, 0.5, 0.7, 0.8, 0.9]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic net seems to give the best scores. use a gridsearchCV to find the best elasticNet parameters\n",
    "param_grid = [\n",
    "    {'alpha' : [0.2, 0.5, 0.7,  1.0], 'l1_ratio': [ 0.1,0.4,  0.5, 0.7, 0.8, 0.9]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 10, cv = 10)\n",
    "grid_search.fit(Xt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.2, 'l1_ratio': 0.9}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the grid serach results to create the final predictor for the test\n",
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "elastic_net.fit(Xt,Y)\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "X_test = test_data.drop(columns = [\"Id\"])\n",
    "\n",
    "# numerical columns\n",
    "X_num_test = ct.fit_transform(X_test)\n",
    "# PCA on the numerical data\n",
    "X_pca_test = pca.transform(X_num_test)\n",
    "# categorical colums that are OHE\n",
    "X_cat_test = cat_pipeline.transform(X_test[categorical_features_names])\n",
    "\n",
    "X_final_test = np.concatenate((X_pca_test, X_cat_test.todense() ), axis = 1)\n",
    "#make predictions which we will submit. \n",
    "y_pred = elastic_net.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the grid serach results to create the final predictor for the test\n",
    "elastic_net= clf = RandomForestRegressor(random_state=1, n_estimators = 500, criterion=\"mae\", n_jobs=-1)\n",
    "elastic_net.fit(Xt,Y)\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "X_test = test_data.drop(columns = [\"Id\"])\n",
    "\n",
    "# numerical columns\n",
    "X_num_test = ct.fit_transform(X_test)\n",
    "# PCA on the numerical data\n",
    "X_pca_test = pca.transform(X_num_test)\n",
    "# categorical colums that are OHE\n",
    "X_cat_test = cat_pipeline.transform(X_test[categorical_features_names])\n",
    "\n",
    "X_final_test = np.concatenate((X_pca_test, X_cat_test.todense() ), axis = 1)\n",
    "#make predictions which we will submit. \n",
    "y_pred = elastic_net.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.1318           20.20s\n",
      "         2           0.1101           18.95s\n",
      "         3           0.0924           18.84s\n",
      "         4           0.0777           19.00s\n",
      "         5           0.0658           21.12s\n",
      "         6           0.0560           20.62s\n",
      "         7           0.0479           20.13s\n",
      "         8           0.0412           19.78s\n",
      "         9           0.0356           19.46s\n",
      "        10           0.0310           20.04s\n",
      "        11           0.0271           19.83s\n",
      "        12           0.0239           19.59s\n",
      "        13           0.0212           19.35s\n",
      "        14           0.0189           19.55s\n",
      "        15           0.0169           20.00s\n",
      "        16           0.0153           19.77s\n",
      "        17           0.0139           19.61s\n",
      "        18           0.0127           19.48s\n",
      "        19           0.0116           19.77s\n",
      "        20           0.0108           19.57s\n",
      "        21           0.0100           19.40s\n",
      "        22           0.0093           19.23s\n",
      "        23           0.0088           19.37s\n",
      "        24           0.0082           19.39s\n",
      "        25           0.0077           19.24s\n",
      "        26           0.0073           19.06s\n",
      "        27           0.0068           18.89s\n",
      "        28           0.0064           18.80s\n",
      "        29           0.0061           18.89s\n",
      "        30           0.0058           18.76s\n",
      "        31           0.0056           18.61s\n",
      "        32           0.0053           18.46s\n",
      "        33           0.0051           18.53s\n",
      "        34           0.0049           18.39s\n",
      "        35           0.0047           18.23s\n",
      "        36           0.0046           18.13s\n",
      "        37           0.0044           17.95s\n",
      "        38           0.0042           17.84s\n",
      "        39           0.0041           17.88s\n",
      "        40           0.0039           17.78s\n",
      "        41           0.0038           17.65s\n",
      "        42           0.0037           17.56s\n",
      "        43           0.0036           17.48s\n",
      "        44           0.0035           17.70s\n",
      "        45           0.0034           17.74s\n",
      "        46           0.0033           17.66s\n",
      "        47           0.0032           17.65s\n",
      "        48           0.0031           17.53s\n",
      "        49           0.0030           17.41s\n",
      "        50           0.0029           17.27s\n",
      "        51           0.0029           17.16s\n",
      "        52           0.0028           17.18s\n",
      "        53           0.0028           17.04s\n",
      "        54           0.0027           16.90s\n",
      "        55           0.0026           16.79s\n",
      "        56           0.0026           16.62s\n",
      "        57           0.0026           16.56s\n",
      "        58           0.0025           16.50s\n",
      "        59           0.0024           16.39s\n",
      "        60           0.0024           16.24s\n",
      "        61           0.0024           16.13s\n",
      "        62           0.0023           15.99s\n",
      "        63           0.0023           16.01s\n",
      "        64           0.0022           15.94s\n",
      "        65           0.0022           15.80s\n",
      "        66           0.0022           15.67s\n",
      "        67           0.0021           15.53s\n",
      "        68           0.0021           15.45s\n",
      "        69           0.0021           15.32s\n",
      "        70           0.0020           15.27s\n",
      "        71           0.0020           15.20s\n",
      "        72           0.0020           15.09s\n",
      "        73           0.0019           15.00s\n",
      "        74           0.0019           14.88s\n",
      "        75           0.0019           14.77s\n",
      "        76           0.0018           14.70s\n",
      "        77           0.0018           14.70s\n",
      "        78           0.0018           14.63s\n",
      "        79           0.0018           14.51s\n",
      "        80           0.0017           14.42s\n",
      "        81           0.0017           14.36s\n",
      "        82           0.0017           14.36s\n",
      "        83           0.0016           14.31s\n",
      "        84           0.0016           14.22s\n",
      "        85           0.0016           14.14s\n",
      "        86           0.0016           14.05s\n",
      "        87           0.0015           13.95s\n",
      "        88           0.0015           13.86s\n",
      "        89           0.0015           13.88s\n",
      "        90           0.0015           13.83s\n",
      "        91           0.0014           13.77s\n",
      "        92           0.0014           13.70s\n",
      "        93           0.0014           13.60s\n",
      "        94           0.0014           13.50s\n",
      "        95           0.0014           13.41s\n",
      "        96           0.0014           13.40s\n",
      "        97           0.0013           13.31s\n",
      "        98           0.0013           13.27s\n",
      "        99           0.0013           13.18s\n",
      "       100           0.0013           13.11s\n",
      "       101           0.0013           13.08s\n",
      "       102           0.0013           13.04s\n",
      "       103           0.0012           12.96s\n",
      "       104           0.0012           12.89s\n",
      "       105           0.0012           12.83s\n",
      "       106           0.0012           12.77s\n",
      "       107           0.0012           12.78s\n",
      "       108           0.0012           12.79s\n",
      "       109           0.0011           12.72s\n",
      "       110           0.0011           12.67s\n",
      "       111           0.0011           12.59s\n",
      "       112           0.0011           12.51s\n",
      "       113           0.0011           12.52s\n",
      "       114           0.0011           12.45s\n",
      "       115           0.0010           12.40s\n",
      "       116           0.0010           12.36s\n",
      "       117           0.0010           12.29s\n",
      "       118           0.0010           12.23s\n",
      "       119           0.0010           12.22s\n",
      "       120           0.0009           12.17s\n",
      "       121           0.0009           12.10s\n",
      "       122           0.0009           12.04s\n",
      "       123           0.0009           12.00s\n",
      "       124           0.0009           11.94s\n",
      "       125           0.0009           11.92s\n",
      "       126           0.0009           11.89s\n",
      "       127           0.0008           11.84s\n",
      "       128           0.0008           11.77s\n",
      "       129           0.0008           11.75s\n",
      "       130           0.0008           11.71s\n",
      "       131           0.0008           11.65s\n",
      "       132           0.0008           11.59s\n",
      "       133           0.0008           11.53s\n",
      "       134           0.0008           11.49s\n",
      "       135           0.0007           11.48s\n",
      "       136           0.0007           11.42s\n",
      "       137           0.0007           11.35s\n",
      "       138           0.0007           11.29s\n",
      "       139           0.0007           11.22s\n",
      "       140           0.0007           11.16s\n",
      "       141           0.0007           11.10s\n",
      "       142           0.0007           11.08s\n",
      "       143           0.0007           11.02s\n",
      "       144           0.0007           10.97s\n",
      "       145           0.0006           10.92s\n",
      "       146           0.0006           10.86s\n",
      "       147           0.0006           10.84s\n",
      "       148           0.0006           10.79s\n",
      "       149           0.0006           10.73s\n",
      "       150           0.0006           10.67s\n",
      "       151           0.0006           10.63s\n",
      "       152           0.0006           10.58s\n",
      "       153           0.0006           10.55s\n",
      "       154           0.0006           10.50s\n",
      "       155           0.0006           10.45s\n",
      "       156           0.0006           10.39s\n",
      "       157           0.0005           10.34s\n",
      "       158           0.0005           10.30s\n",
      "       159           0.0005           10.29s\n",
      "       160           0.0005           10.26s\n",
      "       161           0.0005           10.21s\n",
      "       162           0.0005           10.15s\n",
      "       163           0.0005           10.11s\n",
      "       164           0.0005           10.06s\n",
      "       165           0.0005           10.07s\n",
      "       166           0.0005           10.06s\n",
      "       167           0.0005           10.01s\n",
      "       168           0.0005            9.96s\n",
      "       169           0.0005            9.91s\n",
      "       170           0.0004            9.88s\n",
      "       171           0.0004            9.83s\n",
      "       172           0.0004            9.77s\n",
      "       173           0.0004            9.71s\n",
      "       174           0.0004            9.66s\n",
      "       175           0.0004            9.60s\n",
      "       176           0.0004            9.54s\n",
      "       177           0.0004            9.51s\n",
      "       178           0.0004            9.47s\n",
      "       179           0.0004            9.42s\n",
      "       180           0.0004            9.36s\n",
      "       181           0.0004            9.30s\n",
      "       182           0.0004            9.25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       183           0.0004            9.22s\n",
      "       184           0.0004            9.23s\n",
      "       185           0.0004            9.20s\n",
      "       186           0.0004            9.20s\n",
      "       187           0.0004            9.16s\n",
      "       188           0.0004            9.11s\n",
      "       189           0.0004            9.07s\n",
      "       190           0.0003            9.03s\n",
      "       191           0.0003            8.98s\n",
      "       192           0.0003            8.96s\n",
      "       193           0.0003            8.91s\n",
      "       194           0.0003            8.86s\n",
      "       195           0.0003            8.81s\n",
      "       196           0.0003            8.79s\n",
      "       197           0.0003            8.75s\n",
      "       198           0.0003            8.70s\n",
      "       199           0.0003            8.64s\n",
      "       200           0.0003            8.61s\n",
      "       201           0.0003            8.60s\n",
      "       202           0.0003            8.57s\n",
      "       203           0.0003            8.54s\n",
      "       204           0.0003            8.48s\n",
      "       205           0.0003            8.43s\n",
      "       206           0.0003            8.39s\n",
      "       207           0.0003            8.36s\n",
      "       208           0.0003            8.32s\n",
      "       209           0.0003            8.27s\n",
      "       210           0.0003            8.22s\n",
      "       211           0.0003            8.18s\n",
      "       212           0.0003            8.14s\n",
      "       213           0.0002            8.08s\n",
      "       214           0.0002            8.03s\n",
      "       215           0.0002            7.99s\n",
      "       216           0.0002            7.93s\n",
      "       217           0.0002            7.89s\n",
      "       218           0.0002            7.85s\n",
      "       219           0.0002            7.81s\n",
      "       220           0.0002            7.75s\n",
      "       221           0.0002            7.71s\n",
      "       222           0.0002            7.66s\n",
      "       223           0.0002            7.62s\n",
      "       224           0.0002            7.57s\n",
      "       225           0.0002            7.53s\n",
      "       226           0.0002            7.48s\n",
      "       227           0.0002            7.43s\n",
      "       228           0.0002            7.38s\n",
      "       229           0.0002            7.35s\n",
      "       230           0.0002            7.29s\n",
      "       231           0.0002            7.24s\n",
      "       232           0.0002            7.19s\n",
      "       233           0.0002            7.14s\n",
      "       234           0.0002            7.09s\n",
      "       235           0.0002            7.04s\n",
      "       236           0.0002            7.02s\n",
      "       237           0.0002            6.98s\n",
      "       238           0.0002            6.93s\n",
      "       239           0.0002            6.88s\n",
      "       240           0.0002            6.83s\n",
      "       241           0.0002            6.78s\n",
      "       242           0.0002            6.76s\n",
      "       243           0.0002            6.72s\n",
      "       244           0.0002            6.67s\n",
      "       245           0.0002            6.62s\n",
      "       246           0.0002            6.58s\n",
      "       247           0.0002            6.53s\n",
      "       248           0.0002            6.49s\n",
      "       249           0.0002            6.44s\n",
      "       250           0.0002            6.40s\n",
      "       251           0.0002            6.35s\n",
      "       252           0.0001            6.32s\n",
      "       253           0.0001            6.27s\n",
      "       254           0.0001            6.23s\n",
      "       255           0.0001            6.18s\n",
      "       256           0.0001            6.13s\n",
      "       257           0.0001            6.09s\n",
      "       258           0.0001            6.04s\n",
      "       259           0.0001            6.05s\n",
      "       260           0.0001            6.07s\n",
      "       261           0.0001            6.06s\n",
      "       262           0.0001            6.02s\n",
      "       263           0.0001            5.97s\n",
      "       264           0.0001            5.93s\n",
      "       265           0.0001            5.89s\n",
      "       266           0.0001            5.84s\n",
      "       267           0.0001            5.79s\n",
      "       268           0.0001            5.75s\n",
      "       269           0.0001            5.71s\n",
      "       270           0.0001            5.66s\n",
      "       271           0.0001            5.62s\n",
      "       272           0.0001            5.57s\n",
      "       273           0.0001            5.53s\n",
      "       274           0.0001            5.48s\n",
      "       275           0.0001            5.44s\n",
      "       276           0.0001            5.39s\n",
      "       277           0.0001            5.35s\n",
      "       278           0.0001            5.30s\n",
      "       279           0.0001            5.26s\n",
      "       280           0.0001            5.21s\n",
      "       281           0.0001            5.18s\n",
      "       282           0.0001            5.13s\n",
      "       283           0.0001            5.09s\n",
      "       284           0.0001            5.05s\n",
      "       285           0.0001            5.00s\n",
      "       286           0.0001            4.95s\n",
      "       287           0.0001            4.91s\n",
      "       288           0.0001            4.87s\n",
      "       289           0.0001            4.82s\n",
      "       290           0.0001            4.79s\n",
      "       291           0.0001            4.77s\n",
      "       292           0.0001            4.74s\n",
      "       293           0.0001            4.71s\n",
      "       294           0.0001            4.67s\n",
      "       295           0.0001            4.63s\n",
      "       296           0.0001            4.58s\n",
      "       297           0.0001            4.53s\n",
      "       298           0.0001            4.48s\n",
      "       299           0.0001            4.43s\n",
      "       300           0.0001            4.39s\n",
      "       301           0.0001            4.34s\n",
      "       302           0.0001            4.30s\n",
      "       303           0.0001            4.25s\n",
      "       304           0.0001            4.21s\n",
      "       305           0.0001            4.16s\n",
      "       306           0.0001            4.12s\n",
      "       307           0.0001            4.08s\n",
      "       308           0.0001            4.04s\n",
      "       309           0.0001            3.99s\n",
      "       310           0.0001            3.95s\n",
      "       311           0.0001            3.90s\n",
      "       312           0.0001            3.85s\n",
      "       313           0.0001            3.81s\n",
      "       314           0.0001            3.78s\n",
      "       315           0.0001            3.73s\n",
      "       316           0.0001            3.69s\n",
      "       317           0.0001            3.64s\n",
      "       318           0.0001            3.60s\n",
      "       319           0.0001            3.55s\n",
      "       320           0.0001            3.50s\n",
      "       321           0.0001            3.46s\n",
      "       322           0.0001            3.41s\n",
      "       323           0.0001            3.37s\n",
      "       324           0.0001            3.33s\n",
      "       325           0.0001            3.28s\n",
      "       326           0.0001            3.24s\n",
      "       327           0.0001            3.19s\n",
      "       328           0.0001            3.15s\n",
      "       329           0.0001            3.11s\n",
      "       330           0.0001            3.07s\n",
      "       331           0.0001            3.03s\n",
      "       332           0.0001            2.98s\n",
      "       333           0.0001            2.94s\n",
      "       334           0.0001            2.89s\n",
      "       335           0.0001            2.85s\n",
      "       336           0.0001            2.80s\n",
      "       337           0.0001            2.75s\n",
      "       338           0.0000            2.71s\n",
      "       339           0.0000            2.67s\n",
      "       340           0.0000            2.62s\n",
      "       341           0.0000            2.58s\n",
      "       342           0.0000            2.53s\n",
      "       343           0.0000            2.49s\n",
      "       344           0.0000            2.44s\n",
      "       345           0.0000            2.40s\n",
      "       346           0.0000            2.36s\n",
      "       347           0.0000            2.31s\n",
      "       348           0.0000            2.27s\n",
      "       349           0.0000            2.24s\n",
      "       350           0.0000            2.20s\n",
      "       351           0.0000            2.16s\n",
      "       352           0.0000            2.12s\n",
      "       353           0.0000            2.08s\n",
      "       354           0.0000            2.04s\n",
      "       355           0.0000            2.00s\n",
      "       356           0.0000            1.96s\n",
      "       357           0.0000            1.92s\n",
      "       358           0.0000            1.88s\n",
      "       359           0.0000            1.84s\n",
      "       360           0.0000            1.80s\n",
      "       361           0.0000            1.75s\n",
      "       362           0.0000            1.71s\n",
      "       363           0.0000            1.67s\n",
      "       364           0.0000            1.63s\n",
      "       365           0.0000            1.59s\n",
      "       366           0.0000            1.55s\n",
      "       367           0.0000            1.50s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       368           0.0000            1.46s\n",
      "       369           0.0000            1.42s\n",
      "       370           0.0000            1.37s\n",
      "       371           0.0000            1.33s\n",
      "       372           0.0000            1.28s\n",
      "       373           0.0000            1.24s\n",
      "       374           0.0000            1.20s\n",
      "       375           0.0000            1.15s\n",
      "       376           0.0000            1.11s\n",
      "       377           0.0000            1.06s\n",
      "       378           0.0000            1.02s\n",
      "       379           0.0000            0.97s\n",
      "       380           0.0000            0.93s\n",
      "       381           0.0000            0.89s\n",
      "       382           0.0000            0.84s\n",
      "       383           0.0000            0.79s\n",
      "       384           0.0000            0.75s\n",
      "       385           0.0000            0.70s\n",
      "       386           0.0000            0.66s\n",
      "       387           0.0000            0.61s\n",
      "       388           0.0000            0.56s\n",
      "       389           0.0000            0.52s\n",
      "       390           0.0000            0.47s\n",
      "       391           0.0000            0.42s\n",
      "       392           0.0000            0.38s\n",
      "       393           0.0000            0.33s\n",
      "       394           0.0000            0.28s\n",
      "       395           0.0000            0.23s\n",
      "       396           0.0000            0.19s\n",
      "       397           0.0000            0.14s\n",
      "       398           0.0000            0.09s\n",
      "       399           0.0000            0.05s\n",
      "       400           0.0000            0.00s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "est=GradientBoostingRegressor(n_estimators=400, max_depth=5, loss='ls',min_samples_split=2,learning_rate=0.1, verbose = 5).fit(Xt, np.log(Y))\n",
    "y_pred = est.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': np.exp(y_pred)})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
