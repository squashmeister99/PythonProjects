{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (StandardScaler, OneHotEncoder, FunctionTransformer, KBinsDiscretizer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KitchenQual', 'GarageCond', 'GarageQual', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'LotConfig', 'Utilities', 'LandSlope', 'LotShape', 'GarageType', 'BldgType', 'CentralAir', 'MSZoning', 'LandContour', 'PavedDrive']\n"
     ]
    }
   ],
   "source": [
    "# constants and helper methods\n",
    "\n",
    "CONDITIONS_DICT = {\"NA\": 0, \"NaN\": 0, \"nan\": 0, \"Po\": 2, \"Fa\": 3, \"TA\": 4, \"Gd\":6, \"Ex\": 10}\n",
    "\n",
    "# constants\n",
    "CATEGORY_LABELS = {\"KitchenQual\":       CONDITIONS_DICT,\n",
    "                    \"GarageCond\":       CONDITIONS_DICT,\n",
    "                    \"GarageQual\":       CONDITIONS_DICT,\n",
    "                    \"ExterQual\":        CONDITIONS_DICT,\n",
    "                    \"ExterCond\":        CONDITIONS_DICT,\n",
    "                    \"BsmtQual\":         CONDITIONS_DICT,\n",
    "                    \"BsmtCond\":         CONDITIONS_DICT,\n",
    "                    \"FireplaceQu\" :     CONDITIONS_DICT,\n",
    "                    \"HeatingQC\" :       CONDITIONS_DICT,\n",
    "                    \"LotConfig\":     {\"Inside\": 0, \"Corner\": 6, \"CulDSac\": 10, \"FR2\": 3, \"FR3\":4},\n",
    "                    \"Utilities\":     {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "                    \"LandSlope\":     {\"Gtl\": 10, \"Mod\": 4, \"Sev\": 1},\n",
    "                    \"LotShape\":     {\"Reg\": 10, \"IR1\": 5, \"IR2\": 3, \"IR3\": 1},\n",
    "                    \"GarageType\":     {\"NA\": 0, \"nan\": 0, \"Basment\": 4,  \"Detchd\": 1, \"CarPort\": 3, \"BuiltIn\": 5, \"Attchd\": 7, \"2Types\": 12},\n",
    "                    \"BldgType\":     {\"TwnhsI\": 1, \"Twnhs\": 2, \"TwnhsE\": 3, \"Duplex\": 5,  \"2fmCon\": 7, \"1Fam\": 12},\n",
    "                    \"CentralAir\":     {\"N\": 1, \"Y\": 10},\n",
    "                    \"Electrical\":     {\"Mix\": 1, \"FuseP\": 3, \"FuseF\": 5,  \"FuseA\": 7, \"SBrkr\": 12},\n",
    "                    \"MSZoning\":     {\"RL\": 100, \"RM\": 60, \"C (all)\": 20, \"FV\": 30, \"RH\": 30},\n",
    "                    \"LandContour\":     {\"Lvl\": 100, \"Low\": 15, \"Bnk\": 25, \"HLS\": 5},\n",
    "                    \"Fence\":     {\"NA\": 0, \"MnPrv\": 25, \"MnWw\": 15, \"GdWo\": 40, 'GdPrv': 100},\n",
    "                    \"Functional\":     {\"Typ\": 100, \"Min1\": 70, \"Min2\": 50, \"Mod\": 40, \"Maj1\": 25, \"Maj2\": 20, \"Min2\": 10, \"Sev\": 5, \"Sal\": 1},\n",
    "                    \"MiscFeature\":     {\"NA\": 0, \"Shed\": 30, \"Gar2\": 40, \"Othr\": 25, \"TenC\": 100},\n",
    "                    \"PavedDrive\":     {\"Y\": 100, \"P\": 30, \"N\": 0},\n",
    "                    }\n",
    "\n",
    "CAT_COLS_TO_IGNORE = [\"Functional\",\n",
    "                        \"MiscFeature\",\n",
    "                        \"Electrical\",\n",
    "                        \"Fence\",\n",
    "                        \"FireplaceQu\",\n",
    "                        \"HeatingQC\"           \n",
    "                    ]\n",
    "\n",
    "CAT_COLS = [ x for x in CATEGORY_LABELS.keys() if x not in CAT_COLS_TO_IGNORE]\n",
    "print(CAT_COLS)\n",
    "\n",
    "# plot correlations\n",
    "def plotCoorelations(df):\n",
    "    # remove non_numeric features  \n",
    "    corr = df.corr()\n",
    "    corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierRemoval_IsolationForest(X, y, outlierFraction = 0.02):\n",
    "    clf = IsolationForest( behaviour = 'new', contamination = outlierFraction)\n",
    "    preds = clf.fit_predict(X)\n",
    "    outliers = np.where(preds == -1)\n",
    "    return dropOutliers(X, y, outliers)\n",
    "\n",
    "def dropOutliers(X, y, outliers):\n",
    "    print(\"number of outliers = {0}\".format(len(outliers[0])))\n",
    "    # drop outliers\n",
    "    X_clean = np.delete(X, outliers[0], axis = 0)\n",
    "    y_clean = np.delete(y.values, outliers[0])\n",
    "    return X_clean, y_clean, outliers[0] \n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierRemoval_ZScore(X, y, zValue = 3, bypass=False):\n",
    "    if bypass:\n",
    "        return X, y, []\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    outliers = np.where(z > zValue)\n",
    "    return dropOutliers(X, y, outliers)\n",
    "\n",
    "def getTransformedColumnNames(ct):\n",
    "    for item in ct.named_transformers_:\n",
    "       pipeline = ct.named_transformers_[item]\n",
    "       for step in pipeline.named_steps:\n",
    "           t1 = pipeline.named_steps[step]\n",
    "           print(t1.get_feature_names())\n",
    "            \n",
    "  \n",
    "def displayScoresExp1p(scores):\n",
    "    print(\"Scores:\", np.expm1(scores))\n",
    "    print(\"Mean:\", np.expm1(scores.mean()))\n",
    "    print(\"standard deviation:\", np.expm1(scores.std()))\n",
    "    \n",
    "def displayScores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"standard deviation:\", scores.std())\n",
    "    \n",
    "def columnsWithMissingData(X, threshold = 0.9):\n",
    "    # check for null items\n",
    "    null_df = X.columns[X.isnull().any()]\n",
    "    null_count = X[null_df].isnull().sum()/len(X.index)\n",
    "    null_count_above_threshold = null_count.loc[null_count > threshold]\n",
    "    null_count_above_threshold\n",
    "    \n",
    "    #percentage of zero values for each numeric variable\n",
    "    zero_df = X.columns[(X == 0).any()]\n",
    "    zero_count = (X[zero_df] == 0).sum()/len(X.index)\n",
    "    zero_count_above_threshold = zero_count.loc[zero_count > threshold]\n",
    "    return pd.concat([null_count_above_threshold, zero_count_above_threshold])\n",
    "\n",
    "def crossValidateModel(model, X, y, name=\"<unknown>\"):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(model, X, y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print(\"model {0} cross_val_score took {1} seconds\".format(name, elapsed_time))\n",
    "    displayScores(-scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "iowa_file_path = '../data/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = home_data[\"SalePrice\"]\n",
    "X = home_data.drop(columns = [\"Id\", \"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "following columns have greater than 90% data missing or NULL \n",
      " Alley           0.937371\n",
      "PoolQC          0.995871\n",
      "MiscFeature     0.964212\n",
      "LowQualFinSF    0.982794\n",
      "BsmtHalfBath    0.944253\n",
      "3SsnPorch       0.983482\n",
      "ScreenPorch     0.920853\n",
      "PoolArea        0.995871\n",
      "MiscVal         0.965588\n",
      "dtype: float64\n",
      "original df shape =  (1453, 79)\n",
      "final df shape =  (1453, 70)\n"
     ]
    }
   ],
   "source": [
    "# find missing data.. remove colums that have > 90% data missing\n",
    "missingData = columnsWithMissingData(X, threshold = 0.9)\n",
    "print(\"following columns have greater than 90% data missing or NULL \\n\",missingData)\n",
    "print(\"original df shape = \", X.shape)\n",
    "X = X.drop(columns = missingData.keys())\n",
    "print(\"final df shape = \", X.shape)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature scaling for numerical features\n",
    "numeric_features = X.select_dtypes(exclude=object) \n",
    "num_features_names = numeric_features.columns\n",
    "\n",
    "# features that need a log transformation\n",
    "log_features_names = [\"LotFrontage\", \"LotArea\", \"1stFlrSF\", \"GrLivArea\", \"OpenPorchSF\"]\n",
    "\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('logscaler', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#numeric features that require a normal transformation\n",
    "numeric_features_names = [x for x in num_features_names if x not in log_features_names]\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "transformers=[\n",
    "        ('log', log_pipeline, log_features_names),\n",
    "        ('num', numeric_pipeline, numeric_features_names),  \n",
    "    ]\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "ct = ColumnTransformer(transformers=transformers, sparse_threshold = 0)\n",
    "Xt = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from numeric data\n",
    "Xt, Y, outliers = outlierRemoval_ZScore(Xt, Y, 8, bypass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xt shape:  (1453, 30)\n",
      "X_pca_t shape:  (1453, 26)\n"
     ]
    }
   ],
   "source": [
    "# use PCA to fit and transform the data using a 0.95 variance\n",
    "pca = PCA(0.99)\n",
    "X_pca_t = pca.fit_transform(Xt)\n",
    "print(\"Xt shape: \", Xt.shape)\n",
    "print(\"X_pca_t shape: \", X_pca_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_names = X.select_dtypes(include=object).columns\n",
    "#categorical_features_names = CAT_COLS\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "X_cat_t = cat_pipeline.fit_transform(X[categorical_features_names])\n",
    "\n",
    "# remove corresponding outlier rows from the categorical data\n",
    "X_cat_t2 = np.delete(X_cat_t.todense(), outliers, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated Xt shape:  (1453, 281)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the numerical and one hot encoded categorical data\n",
    "Xt = np.concatenate((X_pca_t, X_cat_t2 ), axis = 1)\n",
    "print(\"concatenated Xt shape: \", Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 2.1349775791168213 seconds\n",
      "Scores: [16328.8001925  16819.2430448  17987.67410304 20633.67588252\n",
      " 21047.51060786 19193.91308135 16735.25650182 16921.5735039\n",
      " 20831.46930085 15829.70679351]\n",
      "Mean: 18232.882301215395\n",
      "standard deviation: 1917.912273329758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    2.0s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha = 0.1, solver=\"auto\")\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(ridge_reg, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model <unknown> cross_val_score took 0.36907958984375 seconds\n",
      "Scores: [16328.8001925  16819.2430448  17987.67410304 20633.67588252\n",
      " 21047.51060786 19193.91308135 16735.25650182 16921.5735039\n",
      " 20831.46930085 15829.70679351]\n",
      "Mean: 18232.882301215395\n",
      "standard deviation: 1917.912273329758\n"
     ]
    }
   ],
   "source": [
    "crossValidateModel(ridge_reg, Xt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    3.2s remaining:    2.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 4.841043710708618 seconds\n",
      "Scores: [16446.5988932  17018.24232882 18126.63417991 20574.5655425\n",
      " 21290.39285962 19659.14925501 16631.38867896 17037.54285451\n",
      " 20974.8755088  15830.44380414]\n",
      "Mean: 18358.983390548485\n",
      "standard deviation: 1966.427645011537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(lasso_reg, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score took 0.7519874572753906 seconds\n",
      "Scores: [16214.69443303 15751.86011982 16250.64452887 19033.42175816\n",
      " 18077.29522209 14639.25458222 14631.87224919 14843.46069906\n",
      " 19075.20971853 15640.61148048]\n",
      "Mean: 16415.83247914543\n",
      "standard deviation: 1631.1096218551395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "# fit the log of the sale price\n",
    "start = time.time()\n",
    "scores = cross_val_score(elastic_net, Xt, Y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = 10)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"cross_val_score took {0} seconds\".format(elapsed_time))\n",
    "displayScores(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic net seems to give the best scores. use a gridsearchCV to find the best elasticNet parameters\n",
    "param_grid = [\n",
    "    {'alpha' : [0.2, 0.5, 0.7,  1.0], 'l1_ratio': [ 0.1,0.4,  0.5, 0.7, 0.8, 0.9]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 10, cv = 10)\n",
    "grid_search.fit(Xt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the grid serach results to create the final predictor for the test\n",
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "elastic_net.fit(Xt,Y)\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "X_test = test_data.drop(columns = [\"Id\"])\n",
    "X_test = test_data.drop(columns = missingData.keys())\n",
    "# numerical columns\n",
    "X_num_test = ct.fit_transform(X_test)\n",
    "# PCA on the numerical data\n",
    "X_pca_test = pca.transform(X_num_test)\n",
    "# categorical colums that are OHE\n",
    "X_cat_test = cat_pipeline.transform(X_test[categorical_features_names])\n",
    "\n",
    "X_final_test = np.concatenate((X_pca_test, X_cat_test.todense() ), axis = 1)\n",
    "#make predictions which we will submit. \n",
    "y_pred = elastic_net.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "est=GradientBoostingRegressor(n_estimators=600, max_depth=5, loss='ls',min_samples_split=2,learning_rate=0.1, verbose = 5).fit(Xt, np.log(Y))\n",
    "y_pred = est.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': np.exp(y_pred)})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "\n",
    "adaboost_model = AdaBoostRegressor(base_estimator= elastic_net, random_state=5, n_estimators=1000)\n",
    "adaboost_model.fit(Xt, np.log(Y))\n",
    "y_pred = adaboost_model.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': np.exp(y_pred)})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
