{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (StandardScaler, RobustScaler, OneHotEncoder, FunctionTransformer, KBinsDiscretizer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import (ElasticNet, Ridge, Lasso)\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import AveragedModels as av\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import xgboost as xgb\n",
    "import ml_helper as mlh\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and helper methods\n",
    "\n",
    "CONDITIONS_DICT = {\"NA\": 0, \"NaN\": 0, \"nan\": 0, \"Po\": 2, \"Fa\": 3, \"TA\": 4, \"Gd\":6, \"Ex\": 10}\n",
    "\n",
    "# constants\n",
    "CATEGORY_LABELS = {\"KitchenQual\":       CONDITIONS_DICT,\n",
    "                    \"GarageCond\":       CONDITIONS_DICT,\n",
    "                    \"GarageQual\":       CONDITIONS_DICT,\n",
    "                    \"ExterQual\":        CONDITIONS_DICT,\n",
    "                    \"ExterCond\":        CONDITIONS_DICT,\n",
    "                    \"BsmtQual\":         CONDITIONS_DICT,\n",
    "                    \"BsmtCond\":         CONDITIONS_DICT,\n",
    "                    \"FireplaceQu\" :     CONDITIONS_DICT,\n",
    "                    \"HeatingQC\" :       CONDITIONS_DICT,\n",
    "                    \"LotConfig\":     {\"Inside\": 0, \"Corner\": 6, \"CulDSac\": 10, \"FR2\": 3, \"FR3\":4},\n",
    "                    \"Utilities\":     {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "                    \"LandSlope\":     {\"Gtl\": 10, \"Mod\": 4, \"Sev\": 1},\n",
    "                    \"LotShape\":     {\"Reg\": 10, \"IR1\": 5, \"IR2\": 3, \"IR3\": 1},\n",
    "                    \"GarageType\":     {\"NA\": 0, \"nan\": 0, \"Basment\": 4,  \"Detchd\": 1, \"CarPort\": 3, \"BuiltIn\": 5, \"Attchd\": 7, \"2Types\": 12},\n",
    "                    \"BldgType\":     {\"TwnhsI\": 1, \"Twnhs\": 2, \"TwnhsE\": 3, \"Duplex\": 5,  \"2fmCon\": 7, \"1Fam\": 12},\n",
    "                    \"CentralAir\":     {\"N\": 1, \"Y\": 10},\n",
    "                    \"Electrical\":     {\"Mix\": 1, \"FuseP\": 3, \"FuseF\": 5,  \"FuseA\": 7, \"SBrkr\": 12},\n",
    "                    \"MSZoning\":     {\"RL\": 100, \"RM\": 60, \"C (all)\": 20, \"FV\": 30, \"RH\": 30},\n",
    "                    \"LandContour\":     {\"Lvl\": 100, \"Low\": 15, \"Bnk\": 25, \"HLS\": 5},\n",
    "                    \"Fence\":     {\"NA\": 0, \"MnPrv\": 25, \"MnWw\": 15, \"GdWo\": 40, 'GdPrv': 100},\n",
    "                    \"Functional\":     {\"Typ\": 100, \"Min1\": 70, \"Min2\": 50, \"Mod\": 40, \"Maj1\": 25, \"Maj2\": 20, \"Min2\": 10, \"Sev\": 5, \"Sal\": 1},\n",
    "                    \"MiscFeature\":     {\"NA\": 0, \"Shed\": 30, \"Gar2\": 40, \"Othr\": 25, \"TenC\": 100},\n",
    "                    \"PavedDrive\":     {\"Y\": 100, \"P\": 30, \"N\": 0},\n",
    "                    }\n",
    "\n",
    "CAT_COLS_TO_IGNORE = [\"Functional\",\n",
    "                        \"MiscFeature\",\n",
    "                        \"Electrical\",\n",
    "                        \"Fence\",\n",
    "                        \"FireplaceQu\",\n",
    "                        \"HeatingQC\"           \n",
    "                    ]\n",
    "\n",
    "CAT_COLS = [ x for x in CATEGORY_LABELS.keys() if x not in CAT_COLS_TO_IGNORE]\n",
    "\n",
    "# plot correlations\n",
    "def plotCoorelations(df):\n",
    "    # remove non_numeric features  \n",
    "    corr = df.corr()\n",
    "    corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_IsolationForest(X, outlierFraction = 0.02):\n",
    "    clf = IsolationForest( behaviour = 'new', contamination = outlierFraction)\n",
    "    preds = clf.fit_predict(X)\n",
    "    return np.where(preds == -1)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_ZScore(X, zValue = 3):\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    return np.where(z > zValue)\n",
    "    \n",
    "def displayScoresExp1p(scores):\n",
    "    print(\"Scores:\", np.expm1(scores))\n",
    "    print(\"Mean:\", np.expm1(scores.mean()))\n",
    "    print(\"standard deviation:\", np.expm1(scores.std()))\n",
    "    \n",
    "def displayScores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"standard deviation:\", scores.std())\n",
    "    \n",
    "def arrayStats(arrayToInspect):\n",
    "    print(\"min:\", np.amin(arrayToInspect, axis=1))\n",
    "    print(\"max:\", np.amax(arrayToInspect, axis=1))\n",
    "    \n",
    "def crossValidateModel(model, X, y, name=\"<unknown>\", crossVal = 5):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(model, X, y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = crossVal)\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print(\"model {0} cross_val_score took {1} seconds\".format(name, elapsed_time))\n",
    "    displayScores(-scores)\n",
    "    \n",
    "def columnsWithMissingData(X, threshold = 0.9):\n",
    "    # check for null items\n",
    "    null_df = X.columns[X.isnull().any()]\n",
    "    null_count = X[null_df].isnull().sum()/len(X.index)\n",
    "    null_count_above_threshold = null_count.loc[null_count > threshold]\n",
    "    null_count_above_threshold\n",
    "    \n",
    "    #percentage of zero values for each numeric variable\n",
    "    zero_df = X.columns[(X == 0).any()]\n",
    "    zero_count = (X[zero_df] == 0).sum()/len(X.index)\n",
    "    zero_count_above_threshold = zero_count.loc[zero_count > threshold]\n",
    "    return pd.concat([null_count_above_threshold, zero_count_above_threshold])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "iowa_file_path = '../data/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers\n",
    "home_data = home_data.drop(home_data['LotFrontage']\n",
    "                                     [home_data['LotFrontage']>200].index)\n",
    "home_data = home_data.drop(home_data['LotArea']\n",
    "                                     [home_data['LotArea']>100000].index)\n",
    "home_data = home_data.drop(home_data['BsmtFinSF1']\n",
    "                                     [home_data['BsmtFinSF1']>4000].index)\n",
    "home_data = home_data.drop(home_data['TotalBsmtSF']\n",
    "                                     [home_data['TotalBsmtSF']>6000].index)\n",
    "home_data = home_data.drop(home_data['1stFlrSF']\n",
    "                                     [home_data['1stFlrSF']>4000].index)\n",
    "home_data = home_data.drop(home_data.LowQualFinSF\n",
    "                                     [home_data['LowQualFinSF']>550].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#home_data.replace(CATEGORY_LABELS, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = home_data[\"SalePrice\"]\n",
    "columns_to_ignore = [\"Id\", \"1stFlrSF\", \"2ndFlrSF\", \"GarageYrBlt\", \"GarageArea\", \"TotalBsmtSF\", \"TotRmsAbvGrd\"]\n",
    "X = home_data.drop(columns = columns_to_ignore)\n",
    "X = X.drop(columns = [\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "following columns have greater than 90% data missing or NULL \n",
      " Alley           0.937371\n",
      "PoolQC          0.995871\n",
      "MiscFeature     0.964212\n",
      "LowQualFinSF    0.982794\n",
      "BsmtHalfBath    0.944253\n",
      "3SsnPorch       0.983482\n",
      "ScreenPorch     0.920853\n",
      "PoolArea        0.995871\n",
      "MiscVal         0.965588\n",
      "dtype: float64\n",
      "original df shape =  (1453, 73)\n",
      "final df shape =  (1453, 64)\n"
     ]
    }
   ],
   "source": [
    "# find missing data.. remove colums that have > 90% data missing\n",
    "missingData = columnsWithMissingData(X, threshold = 0.9)\n",
    "print(\"following columns have greater than 90% data missing or NULL \\n\",missingData)\n",
    "print(\"original df shape = \", X.shape)\n",
    "X = X.drop(columns = missingData.keys())\n",
    "print(\"final df shape = \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1453, 64)\n"
     ]
    }
   ],
   "source": [
    "# perform feature scaling for numerical features\n",
    "# Steps: 1. scale columns requiring log tranformations & feature scaling\n",
    "#        2. scale columns requiring normal numerical & feature scaling\n",
    "#        3. concatenate the results of 1 & 2 and then apply PCA for dimensionality reduction\n",
    "#        \n",
    "numeric_features = X.select_dtypes(exclude=object) \n",
    "num_features_names = numeric_features.columns\n",
    "\n",
    "# features that need a log transformation\n",
    "log_features_names = [\"LotFrontage\", \"LotArea\", \"GrLivArea\", \"OpenPorchSF\"]\n",
    "\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 0) ),\n",
    "    ('logscaler', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "#numeric features that require a normal transformation\n",
    "numeric_features_names = [x for x in num_features_names if x not in log_features_names]\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "num_transformers=[\n",
    "        ('log', log_pipeline, log_features_names),\n",
    "        ('num', numeric_pipeline, numeric_features_names),  \n",
    "    ]\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "num_col_transformer = ColumnTransformer(transformers=num_transformers, sparse_threshold = 0)\n",
    "\n",
    "pca_pipeline = Pipeline(steps = [\n",
    "                       ('num_ct', num_col_transformer), # apply the columntransformation\n",
    "                     # ('pca', PCA(0.99)) # apply PCA to data (log + numeric)\n",
    "                       ])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_pca_t = pca_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_names = X.select_dtypes(include=object).columns\n",
    "#cat_features_names = CAT_COLS\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "X_cat_t = cat_pipeline.fit_transform(X[cat_features_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated Xt shape:  (1453, 279)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the numerical and one hot encoded categorical data\n",
    "Xt = np.concatenate((X_pca_t, X_cat_t.todense() ), axis = 1)\n",
    "print(\"concatenated Xt shape: \", Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#rf = RandomForestRegressor(random_state=1, n_estimators = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   20.7s remaining:   31.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model xgb cross_val_score took 21.606999397277832 seconds\n",
      "Scores: [14272.44022498 16497.28641806 16828.97477717 13679.10806843\n",
      " 14966.9826778 ]\n",
      "Mean: 15248.958433287786\n",
      "standard deviation: 1229.0079503730358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   21.5s finished\n"
     ]
    }
   ],
   "source": [
    "# try xgboost\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.8, subsample=0.5,\n",
    "                             learning_rate=0.05, max_depth=5, \n",
    "                             min_child_weight=1.8, n_estimators=700,\n",
    "                             reg_alpha=0.9, reg_lambda=0.9, gamma=0.01, \n",
    "                             silent=1, random_state =7, nthread = -1, refit = True)\n",
    "\n",
    "crossValidateModel(xgb_model, Xt, Y, \"xgb\", crossVal = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model lightgbm cross_val_score took 3.2480008602142334 seconds\n",
      "Scores: [15064.26461589 15890.98702418 15978.90011449 13390.03451107\n",
      " 15923.23113996]\n",
      "Mean: 15249.483481118408\n",
      "standard deviation: 988.8748989468207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.8s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.8s finished\n"
     ]
    }
   ],
   "source": [
    "# try lightgbm\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "crossValidateModel(model_lgb, Xt, Y, \"lightgbm\", crossVal = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   17.2s remaining:   25.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model gboost cross_val_score took 18.118000984191895 seconds\n",
      "Scores: [13951.60447371 16570.35474789 15780.72794483 12958.34744944\n",
      " 15740.82824381]\n",
      "Mean: 15000.37257193725\n",
      "standard deviation: 1333.555718189555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   17.7s finished\n"
     ]
    }
   ],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "crossValidateModel(GBoost, Xt, Y, \"gboost\", crossVal = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.05, loss='huber', max_depth=4,\n",
      "                          max_features='sqrt', max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=15, min_samples_split=10,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
      "                          n_iter_no_change=None, presort='auto', random_state=5,\n",
      "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "                          verbose=0, warm_start=False), LGBMRegressor(bagging_fraction=0.8, bagging_freq=5, bagging_seed=9,\n",
      "              boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "              feature_fraction=0.2319, feature_fraction_seed=9,\n",
      "              importance_type='split', learning_rate=0.05, max_bin=55,\n",
      "              max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
      "              min_data_in_leaf=6, min_split_gain=0.0,\n",
      "              min_sum_hessian_in_leaf=11, n_estimators=720, n_jobs=-1,\n",
      "              num_leaves=5, objective='regression', random_state=None,\n",
      "              reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "              subsample_for_bin=200000, subsample_freq=0), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8, gamma=0.01,\n",
      "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "             max_depth=5, min_child_weight=1.8, missing=None, n_estimators=700,\n",
      "             n_jobs=1, nthread=-1, objective='reg:linear', random_state=7,\n",
      "             refit=True, reg_alpha=0.9, reg_lambda=0.9, scale_pos_weight=1,\n",
      "             seed=None, silent=1, subsample=0.5, verbosity=1))\n"
     ]
    }
   ],
   "source": [
    "averaged_models = av.AveragingModels(models = (GBoost, model_lgb, xgb_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model elasticnet cross_val_score took 2.760103702545166 seconds\n",
      "Scores: [16030.8790045  18247.91017923 16607.89928757 15009.87747313\n",
      " 17090.26522252]\n",
      "Mean: 16597.36623338838\n",
      "standard deviation: 1077.8111170617822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.6s remaining:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9, max_iter = 5000)\n",
    "crossValidateModel(elastic_net, Xt, Y, \"elasticnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    7.1s remaining:   10.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model lasso cross_val_score took 8.222692728042603 seconds\n",
      "Scores: [17501.7688862  20610.35382631 20272.59015596 17575.72286984\n",
      " 18581.49186997]\n",
      "Mean: 18908.3855216559\n",
      "standard deviation: 1312.9485866954244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.8s finished\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso(alpha = 0.1, max_iter = 5000)\n",
    "crossValidateModel(lasso_reg, Xt, Y, \"lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = av.StackingAveragedModels(base_models = (GBoost, model_lgb, xgb_model),\n",
    "                                                 meta_model = lasso_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 80)\n",
      "(1459, 64)\n",
      "[GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.05, loss='huber', max_depth=4,\n",
      "                          max_features='sqrt', max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=15, min_samples_split=10,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
      "                          n_iter_no_change=None, presort='auto', random_state=5,\n",
      "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "                          verbose=0, warm_start=False), LGBMRegressor(bagging_fraction=0.8, bagging_freq=5, bagging_seed=9,\n",
      "              boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "              feature_fraction=0.2319, feature_fraction_seed=9,\n",
      "              importance_type='split', learning_rate=0.05, max_bin=55,\n",
      "              max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
      "              min_data_in_leaf=6, min_split_gain=0.0,\n",
      "              min_sum_hessian_in_leaf=11, n_estimators=720, n_jobs=-1,\n",
      "              num_leaves=5, objective='regression', random_state=None,\n",
      "              reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "              subsample_for_bin=200000, subsample_freq=0), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8, gamma=0.01,\n",
      "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "             max_depth=5, min_child_weight=1.8, missing=None, n_estimators=700,\n",
      "             n_jobs=1, nthread=-1, objective='reg:linear', random_state=7,\n",
      "             refit=True, reg_alpha=0.9, reg_lambda=0.9, scale_pos_weight=1,\n",
      "             seed=None, silent=1, subsample=0.5, verbosity=1)]\n",
      "[120095.87088102 157672.46515874 184804.05875779 ... 164430.1765414\n",
      " 115059.27870678 220461.60383537]\n"
     ]
    }
   ],
   "source": [
    "# use the grid serach results to create the final predictor for the test\n",
    "averaged_models.fit(Xt,np.log1p(Y.values))\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "print(test_data.shape)\n",
    "X_test = test_data.drop(columns = columns_to_ignore)\n",
    "X_test = X_test.drop(columns = missingData.keys())\n",
    "print(X_test.shape)\n",
    "X_pca_test = pca_pipeline.transform(X_test)\n",
    "# categorical colums that are OHE\n",
    "X_cat_test = cat_pipeline.transform(X_test[cat_features_names])\n",
    "\n",
    "X_final_test = np.concatenate((X_pca_test, X_cat_test.todense() ), axis = 1)\n",
    "#make predictions which we will submit. \n",
    "y_pred = np.expm1(averaged_models.predict(X_final_test))\n",
    "print(y_pred)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other models that are not as good.\n",
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9, max_iter = 5000)\n",
    "crossValidateModel(elastic_net, Xt, Y, \"elasticnet\")\n",
    "\n",
    "ridge_reg = Ridge(alpha = 0.1, solver=\"auto\", max_iter = 5000)\n",
    "crossValidateModel(ridge_reg, Xt, Y, \"ridge\")\n",
    "\n",
    "lasso_reg = Lasso(alpha = 0.1, max_iter = 5000)\n",
    "crossValidateModel(lasso_reg, Xt, Y, \"lasso\")\n",
    "\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "crossValidateModel(KRR, Xt, Y, \"krr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
