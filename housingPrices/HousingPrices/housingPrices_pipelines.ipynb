{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (StandardScaler, RobustScaler, OneHotEncoder, FunctionTransformer, KBinsDiscretizer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import (ElasticNet, Ridge, Lasso)\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and helper methods\n",
    "\n",
    "CONDITIONS_DICT = {\"NA\": 0, \"NaN\": 0, \"nan\": 0, \"Po\": 2, \"Fa\": 3, \"TA\": 4, \"Gd\":6, \"Ex\": 10}\n",
    "\n",
    "# constants\n",
    "CATEGORY_LABELS = {\"KitchenQual\":       CONDITIONS_DICT,\n",
    "                    \"GarageCond\":       CONDITIONS_DICT,\n",
    "                    \"GarageQual\":       CONDITIONS_DICT,\n",
    "                    \"ExterQual\":        CONDITIONS_DICT,\n",
    "                    \"ExterCond\":        CONDITIONS_DICT,\n",
    "                    \"BsmtQual\":         CONDITIONS_DICT,\n",
    "                    \"BsmtCond\":         CONDITIONS_DICT,\n",
    "                    \"FireplaceQu\" :     CONDITIONS_DICT,\n",
    "                    \"HeatingQC\" :       CONDITIONS_DICT,\n",
    "                    \"LotConfig\":     {\"Inside\": 0, \"Corner\": 6, \"CulDSac\": 10, \"FR2\": 3, \"FR3\":4},\n",
    "                    \"Utilities\":     {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "                    \"LandSlope\":     {\"Gtl\": 10, \"Mod\": 4, \"Sev\": 1},\n",
    "                    \"LotShape\":     {\"Reg\": 10, \"IR1\": 5, \"IR2\": 3, \"IR3\": 1},\n",
    "                    \"GarageType\":     {\"NA\": 0, \"nan\": 0, \"Basment\": 4,  \"Detchd\": 1, \"CarPort\": 3, \"BuiltIn\": 5, \"Attchd\": 7, \"2Types\": 12},\n",
    "                    \"BldgType\":     {\"TwnhsI\": 1, \"Twnhs\": 2, \"TwnhsE\": 3, \"Duplex\": 5,  \"2fmCon\": 7, \"1Fam\": 12},\n",
    "                    \"CentralAir\":     {\"N\": 1, \"Y\": 10},\n",
    "                    \"Electrical\":     {\"Mix\": 1, \"FuseP\": 3, \"FuseF\": 5,  \"FuseA\": 7, \"SBrkr\": 12},\n",
    "                    \"MSZoning\":     {\"RL\": 100, \"RM\": 60, \"C (all)\": 20, \"FV\": 30, \"RH\": 30},\n",
    "                    \"LandContour\":     {\"Lvl\": 100, \"Low\": 15, \"Bnk\": 25, \"HLS\": 5},\n",
    "                    \"Fence\":     {\"NA\": 0, \"MnPrv\": 25, \"MnWw\": 15, \"GdWo\": 40, 'GdPrv': 100},\n",
    "                    \"Functional\":     {\"Typ\": 100, \"Min1\": 70, \"Min2\": 50, \"Mod\": 40, \"Maj1\": 25, \"Maj2\": 20, \"Min2\": 10, \"Sev\": 5, \"Sal\": 1},\n",
    "                    \"MiscFeature\":     {\"NA\": 0, \"Shed\": 30, \"Gar2\": 40, \"Othr\": 25, \"TenC\": 100},\n",
    "                    \"PavedDrive\":     {\"Y\": 100, \"P\": 30, \"N\": 0},\n",
    "                    }\n",
    "\n",
    "CAT_COLS_TO_IGNORE = [\"Functional\",\n",
    "                        \"MiscFeature\",\n",
    "                        \"Electrical\",\n",
    "                        \"Fence\",\n",
    "                        \"FireplaceQu\",\n",
    "                        \"HeatingQC\"           \n",
    "                    ]\n",
    "\n",
    "CAT_COLS = [ x for x in CATEGORY_LABELS.keys() if x not in CAT_COLS_TO_IGNORE]\n",
    "\n",
    "# plot correlations\n",
    "def plotCoorelations(df):\n",
    "    # remove non_numeric features  \n",
    "    corr = df.corr()\n",
    "    corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_IsolationForest(X, outlierFraction = 0.02):\n",
    "    clf = IsolationForest( behaviour = 'new', contamination = outlierFraction)\n",
    "    preds = clf.fit_predict(X)\n",
    "    return np.where(preds == -1)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_ZScore(X, zValue = 3):\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    return np.where(z > zValue)\n",
    "    \n",
    "def displayScoresExp1p(scores):\n",
    "    print(\"Scores:\", np.expm1(scores))\n",
    "    print(\"Mean:\", np.expm1(scores.mean()))\n",
    "    print(\"standard deviation:\", np.expm1(scores.std()))\n",
    "    \n",
    "def displayScores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"standard deviation:\", scores.std())\n",
    "    \n",
    "def arrayStats(arrayToInspect):\n",
    "    print(\"min:\", np.amin(arrayToInspect, axis=1))\n",
    "    print(\"max:\", np.amax(arrayToInspect, axis=1))\n",
    "    \n",
    "def crossValidateModel(model, X, y, name=\"<unknown>\", crossVal = 10):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(model, X, y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = crossVal)\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print(\"model {0} cross_val_score took {1} seconds\".format(name, elapsed_time))\n",
    "    displayScores(-scores)\n",
    "    \n",
    "def columnsWithMissingData(X, threshold = 0.9):\n",
    "    # check for null items\n",
    "    null_df = X.columns[X.isnull().any()]\n",
    "    null_count = X[null_df].isnull().sum()/len(X.index)\n",
    "    null_count_above_threshold = null_count.loc[null_count > threshold]\n",
    "    null_count_above_threshold\n",
    "    \n",
    "    #percentage of zero values for each numeric variable\n",
    "    zero_df = X.columns[(X == 0).any()]\n",
    "    zero_count = (X[zero_df] == 0).sum()/len(X.index)\n",
    "    zero_count_above_threshold = zero_count.loc[zero_count > threshold]\n",
    "    return pd.concat([null_count_above_threshold, zero_count_above_threshold])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "iowa_file_path = '../data/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers\n",
    "home_data = home_data.drop(home_data['LotFrontage']\n",
    "                                     [home_data['LotFrontage']>200].index)\n",
    "home_data = home_data.drop(home_data['LotArea']\n",
    "                                     [home_data['LotArea']>100000].index)\n",
    "home_data = home_data.drop(home_data['BsmtFinSF1']\n",
    "                                     [home_data['BsmtFinSF1']>4000].index)\n",
    "home_data = home_data.drop(home_data['TotalBsmtSF']\n",
    "                                     [home_data['TotalBsmtSF']>6000].index)\n",
    "home_data = home_data.drop(home_data['1stFlrSF']\n",
    "                                     [home_data['1stFlrSF']>4000].index)\n",
    "home_data = home_data.drop(home_data.LowQualFinSF\n",
    "                                     [home_data['LowQualFinSF']>550].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = home_data[\"SalePrice\"]\n",
    "columns_to_ignore = [\"Id\", \"1stFlrSF\", \"2ndFlrSF\", \"GarageYrBlt\", \"GarageArea\", \"TotalBsmtSF\", \"TotRmsAbvGrd\"]\n",
    "X = home_data.drop(columns = columns_to_ignore)\n",
    "X = X.drop(columns = [\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "following columns have greater than 90% data missing or NULL \n",
      " Alley           0.937371\n",
      "PoolQC          0.995871\n",
      "MiscFeature     0.964212\n",
      "LowQualFinSF    0.982794\n",
      "BsmtHalfBath    0.944253\n",
      "3SsnPorch       0.983482\n",
      "ScreenPorch     0.920853\n",
      "PoolArea        0.995871\n",
      "MiscVal         0.965588\n",
      "dtype: float64\n",
      "original df shape =  (1453, 73)\n",
      "final df shape =  (1453, 64)\n"
     ]
    }
   ],
   "source": [
    "# find missing data.. remove colums that have > 90% data missing\n",
    "missingData = columnsWithMissingData(X, threshold = 0.9)\n",
    "print(\"following columns have greater than 90% data missing or NULL \\n\",missingData)\n",
    "print(\"original df shape = \", X.shape)\n",
    "X = X.drop(columns = missingData.keys())\n",
    "print(\"final df shape = \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1453, 64)\n"
     ]
    }
   ],
   "source": [
    "# perform feature scaling for numerical features\n",
    "# Steps: 1. scale columns requiring log tranformations & feature scaling\n",
    "#        2. scale columns requiring normal numerical & feature scaling\n",
    "#        3. concatenate the results of 1 & 2 and then apply PCA for dimensionality reduction\n",
    "#        \n",
    "numeric_features = X.select_dtypes(exclude=object) \n",
    "num_features_names = numeric_features.columns\n",
    "\n",
    "# features that need a log transformation\n",
    "log_features_names = [\"LotFrontage\", \"LotArea\", \"GrLivArea\", \"OpenPorchSF\"]\n",
    "\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 0) ),\n",
    "    ('logscaler', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "#numeric features that require a normal transformation\n",
    "numeric_features_names = [x for x in num_features_names if x not in log_features_names]\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "num_transformers=[\n",
    "        ('log', log_pipeline, log_features_names),\n",
    "        ('num', numeric_pipeline, numeric_features_names),  \n",
    "    ]\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "num_col_transformer = ColumnTransformer(transformers=num_transformers, sparse_threshold = 0)\n",
    "\n",
    "pca_pipeline = Pipeline(steps = [\n",
    "                       ('num_ct', num_col_transformer), # apply the columntransformation\n",
    "                      # ('pca', PCA(0.99)) # apply PCA to data (log + numeric)\n",
    "                       ])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_pca_t = pca_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_names = X.select_dtypes(include=object).columns\n",
    "#cat_features_names = CAT_COLS\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "X_cat_t = cat_pipeline.fit_transform(X[cat_features_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated Xt shape:  (1453, 279)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the numerical and one hot encoded categorical data\n",
    "Xt = np.concatenate((X_pca_t, X_cat_t.todense() ), axis = 1)\n",
    "print(\"concatenated Xt shape: \", Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    2.8s remaining:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    2.9s remaining:    2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model elasticnet cross_val_score took 3.3927183151245117 seconds\n",
      "Scores: [16236.80204984 15753.82340599 16529.56091252 19506.05756447\n",
      " 18495.21246822 14542.5021054  15339.03756253 14781.65889051\n",
      " 18917.15026522 15382.26955008]\n",
      "Mean: 16548.407477478064\n",
      "standard deviation: 1698.3819733469888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    3.3s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.3s finished\n"
     ]
    }
   ],
   "source": [
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "crossValidateModel(elastic_net, Xt, Y, \"elasticnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic net seems to give the best scores. use a gridsearchCV to find the best elasticNet parameters\n",
    "param_grid = [\n",
    "    {'alpha' : [0.2, 0.5, 0.7,  1.0], 'l1_ratio': [ 0.1,0.4,  0.5, 0.7, 0.8, 0.9]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 10, cv = 10)\n",
    "grid_search.fit(Xt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ridge cross_val_score took 0.263669490814209 seconds\n",
      "Scores: [16560.89418731 17344.85348149 18489.43993088 21188.41763533\n",
      " 21404.17628041 18707.85496801 17117.2491084  16911.16276057\n",
      " 21166.01485479 16126.7960025 ]\n",
      "Mean: 18501.68592096879\n",
      "standard deviation: 1948.7366954658362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    2.5s remaining:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    2.9s remaining:    2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model lasso cross_val_score took 4.750716686248779 seconds\n",
      "Scores: [16689.53857017 17534.13693141 18665.79967919 21212.99524361\n",
      " 21669.1024858  19150.22856014 17179.49191635 17004.85130203\n",
      " 21122.16463788 16242.46366356]\n",
      "Mean: 18647.077299014658\n",
      "standard deviation: 1944.2122861255384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    4.5s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.6s finished\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = Ridge(alpha = 0.1, solver=\"auto\")\n",
    "crossValidateModel(ridge_reg, Xt, Y, \"ridge\")\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "crossValidateModel(lasso_reg, Xt, Y, \"lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ElasticNet(alpha=0.2, copy_X=True, fit_intercept=True, l1_ratio=0.9,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False), Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001), Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
      "                      oob_score=False, random_state=1, verbose=0,\n",
      "                      warm_start=False)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 246644542555.24, tolerance: 905296277.1952931\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "import AveragedModels as av\n",
    "models = [elastic_net, ridge_reg, lasso_reg, clf]\n",
    "avmodel = av.AveragingModels(models)\n",
    "avmodel.fit(Xt,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the grid serach results to create the final predictor for the test\n",
    "\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "print(test_data.shape)\n",
    "X_test = test_data.drop(columns = columns_to_ignore)\n",
    "X_test = X_test.drop(columns = missingData.keys())\n",
    "print(X_test.shape)\n",
    "X_pca_test = pca_pipeline.transform(X_test)\n",
    "# categorical colums that are OHE\n",
    "X_cat_test = cat_pipeline.transform(X_test[cat_features_names])\n",
    "\n",
    "X_final_test = np.concatenate((X_pca_test, X_cat_test.todense() ), axis = 1)\n",
    "#make predictions which we will submit. \n",
    "y_pred = avmodel.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
