{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (StandardScaler, RobustScaler, OneHotEncoder, FunctionTransformer, KBinsDiscretizer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import (ElasticNet, Ridge, Lasso)\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import AveragedModels as av\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and helper methods\n",
    "\n",
    "CONDITIONS_DICT = {\"NA\": 0, \"NaN\": 0, \"nan\": 0, \"Po\": 2, \"Fa\": 3, \"TA\": 4, \"Gd\":6, \"Ex\": 10}\n",
    "\n",
    "# constants\n",
    "CATEGORY_LABELS = {\"KitchenQual\":       CONDITIONS_DICT,\n",
    "                    \"GarageCond\":       CONDITIONS_DICT,\n",
    "                    \"GarageQual\":       CONDITIONS_DICT,\n",
    "                    \"ExterQual\":        CONDITIONS_DICT,\n",
    "                    \"ExterCond\":        CONDITIONS_DICT,\n",
    "                    \"BsmtQual\":         CONDITIONS_DICT,\n",
    "                    \"BsmtCond\":         CONDITIONS_DICT,\n",
    "                    \"FireplaceQu\" :     CONDITIONS_DICT,\n",
    "                    \"HeatingQC\" :       CONDITIONS_DICT,\n",
    "                    \"LotConfig\":     {\"Inside\": 0, \"Corner\": 6, \"CulDSac\": 10, \"FR2\": 3, \"FR3\":4},\n",
    "                    \"Utilities\":     {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "                    \"LandSlope\":     {\"Gtl\": 10, \"Mod\": 4, \"Sev\": 1},\n",
    "                    \"LotShape\":     {\"Reg\": 10, \"IR1\": 5, \"IR2\": 3, \"IR3\": 1},\n",
    "                    \"GarageType\":     {\"NA\": 0, \"nan\": 0, \"Basment\": 4,  \"Detchd\": 1, \"CarPort\": 3, \"BuiltIn\": 5, \"Attchd\": 7, \"2Types\": 12},\n",
    "                    \"BldgType\":     {\"TwnhsI\": 1, \"Twnhs\": 2, \"TwnhsE\": 3, \"Duplex\": 5,  \"2fmCon\": 7, \"1Fam\": 12},\n",
    "                    \"CentralAir\":     {\"N\": 1, \"Y\": 10},\n",
    "                    \"Electrical\":     {\"Mix\": 1, \"FuseP\": 3, \"FuseF\": 5,  \"FuseA\": 7, \"SBrkr\": 12},\n",
    "                    \"MSZoning\":     {\"RL\": 100, \"RM\": 60, \"C (all)\": 20, \"FV\": 30, \"RH\": 30},\n",
    "                    \"LandContour\":     {\"Lvl\": 100, \"Low\": 15, \"Bnk\": 25, \"HLS\": 5},\n",
    "                    \"Fence\":     {\"NA\": 0, \"MnPrv\": 25, \"MnWw\": 15, \"GdWo\": 40, 'GdPrv': 100},\n",
    "                    \"Functional\":     {\"Typ\": 100, \"Min1\": 70, \"Min2\": 50, \"Mod\": 40, \"Maj1\": 25, \"Maj2\": 20, \"Min2\": 10, \"Sev\": 5, \"Sal\": 1},\n",
    "                    \"MiscFeature\":     {\"NA\": 0, \"Shed\": 30, \"Gar2\": 40, \"Othr\": 25, \"TenC\": 100},\n",
    "                    \"PavedDrive\":     {\"Y\": 100, \"P\": 30, \"N\": 0},\n",
    "                    }\n",
    "\n",
    "CAT_COLS_TO_IGNORE = [\"Functional\",\n",
    "                        \"MiscFeature\",\n",
    "                        \"Electrical\",\n",
    "                        \"Fence\",\n",
    "                        \"FireplaceQu\",\n",
    "                        \"HeatingQC\"           \n",
    "                    ]\n",
    "\n",
    "CAT_COLS = [ x for x in CATEGORY_LABELS.keys() if x not in CAT_COLS_TO_IGNORE]\n",
    "\n",
    "# plot correlations\n",
    "def plotCoorelations(df):\n",
    "    # remove non_numeric features  \n",
    "    corr = df.corr()\n",
    "    corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_IsolationForest(X, outlierFraction = 0.02):\n",
    "    clf = IsolationForest( behaviour = 'new', contamination = outlierFraction)\n",
    "    preds = clf.fit_predict(X)\n",
    "    return np.where(preds == -1)\n",
    "\n",
    "# define a method to use Isolation Forest for outlier detection\n",
    "def outlierDetect_ZScore(X, zValue = 3):\n",
    "    z = np.abs(stats.zscore(X))\n",
    "    return np.where(z > zValue)\n",
    "    \n",
    "def displayScoresExp1p(scores):\n",
    "    print(\"Scores:\", np.expm1(scores))\n",
    "    print(\"Mean:\", np.expm1(scores.mean()))\n",
    "    print(\"standard deviation:\", np.expm1(scores.std()))\n",
    "    \n",
    "def displayScores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"standard deviation:\", scores.std())\n",
    "    \n",
    "def arrayStats(arrayToInspect):\n",
    "    print(\"min:\", np.amin(arrayToInspect, axis=1))\n",
    "    print(\"max:\", np.amax(arrayToInspect, axis=1))\n",
    "    \n",
    "def crossValidateModel(model, X, y, name=\"<unknown>\", crossVal = 10):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(model, X, y, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 4, cv = crossVal)\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print(\"model {0} cross_val_score took {1} seconds\".format(name, elapsed_time))\n",
    "    displayScores(-scores)\n",
    "    \n",
    "def columnsWithMissingData(X, threshold = 0.9):\n",
    "    # check for null items\n",
    "    null_df = X.columns[X.isnull().any()]\n",
    "    null_count = X[null_df].isnull().sum()/len(X.index)\n",
    "    null_count_above_threshold = null_count.loc[null_count > threshold]\n",
    "    null_count_above_threshold\n",
    "    \n",
    "    #percentage of zero values for each numeric variable\n",
    "    zero_df = X.columns[(X == 0).any()]\n",
    "    zero_count = (X[zero_df] == 0).sum()/len(X.index)\n",
    "    zero_count_above_threshold = zero_count.loc[zero_count > threshold]\n",
    "    return pd.concat([null_count_above_threshold, zero_count_above_threshold])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "iowa_file_path = '../data/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers\n",
    "home_data = home_data.drop(home_data['LotFrontage']\n",
    "                                     [home_data['LotFrontage']>200].index)\n",
    "home_data = home_data.drop(home_data['LotArea']\n",
    "                                     [home_data['LotArea']>100000].index)\n",
    "home_data = home_data.drop(home_data['BsmtFinSF1']\n",
    "                                     [home_data['BsmtFinSF1']>4000].index)\n",
    "home_data = home_data.drop(home_data['TotalBsmtSF']\n",
    "                                     [home_data['TotalBsmtSF']>6000].index)\n",
    "home_data = home_data.drop(home_data['1stFlrSF']\n",
    "                                     [home_data['1stFlrSF']>4000].index)\n",
    "home_data = home_data.drop(home_data.LowQualFinSF\n",
    "                                     [home_data['LowQualFinSF']>550].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = home_data[\"SalePrice\"]\n",
    "columns_to_ignore = [\"Id\", \"1stFlrSF\", \"2ndFlrSF\", \"GarageYrBlt\", \"GarageArea\", \"TotalBsmtSF\", \"TotRmsAbvGrd\"]\n",
    "X = home_data.drop(columns = columns_to_ignore)\n",
    "X = X.drop(columns = [\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "following columns have greater than 90% data missing or NULL \n",
      " Alley           0.937371\n",
      "PoolQC          0.995871\n",
      "MiscFeature     0.964212\n",
      "LowQualFinSF    0.982794\n",
      "BsmtHalfBath    0.944253\n",
      "3SsnPorch       0.983482\n",
      "ScreenPorch     0.920853\n",
      "PoolArea        0.995871\n",
      "MiscVal         0.965588\n",
      "dtype: float64\n",
      "original df shape =  (1453, 73)\n",
      "final df shape =  (1453, 64)\n"
     ]
    }
   ],
   "source": [
    "# find missing data.. remove colums that have > 90% data missing\n",
    "missingData = columnsWithMissingData(X, threshold = 0.9)\n",
    "print(\"following columns have greater than 90% data missing or NULL \\n\",missingData)\n",
    "print(\"original df shape = \", X.shape)\n",
    "X = X.drop(columns = missingData.keys())\n",
    "print(\"final df shape = \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1453, 64)\n"
     ]
    }
   ],
   "source": [
    "# perform feature scaling for numerical features\n",
    "# Steps: 1. scale columns requiring log tranformations & feature scaling\n",
    "#        2. scale columns requiring normal numerical & feature scaling\n",
    "#        3. concatenate the results of 1 & 2 and then apply PCA for dimensionality reduction\n",
    "#        \n",
    "numeric_features = X.select_dtypes(exclude=object) \n",
    "num_features_names = numeric_features.columns\n",
    "\n",
    "# features that need a log transformation\n",
    "log_features_names = [\"LotFrontage\", \"LotArea\", \"GrLivArea\", \"OpenPorchSF\"]\n",
    "\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 0) ),\n",
    "    ('logscaler', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "#numeric features that require a normal transformation\n",
    "numeric_features_names = [x for x in num_features_names if x not in log_features_names]\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "num_transformers=[\n",
    "        ('log', log_pipeline, log_features_names),\n",
    "        ('num', numeric_pipeline, numeric_features_names),  \n",
    "    ]\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "num_col_transformer = ColumnTransformer(transformers=num_transformers, sparse_threshold = 0)\n",
    "\n",
    "pca_pipeline = Pipeline(steps = [\n",
    "                       ('num_ct', num_col_transformer), # apply the columntransformation\n",
    "                      # ('pca', PCA(0.99)) # apply PCA to data (log + numeric)\n",
    "                       ])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_pca_t = pca_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_names = X.select_dtypes(include=object).columns\n",
    "#cat_features_names = CAT_COLS\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# ensure that result is always a dense matrix\n",
    "X_cat_t = cat_pipeline.fit_transform(X[cat_features_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated Xt shape:  (1453, 279)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the numerical and one hot encoded categorical data\n",
    "Xt = np.concatenate((X_pca_t, X_cat_t.todense() ), axis = 1)\n",
    "print(\"concatenated Xt shape: \", Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    2.7s remaining:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    2.9s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    3.2s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model elasticnet cross_val_score took 3.379683494567871 seconds\n",
      "Scores: [16236.80204984 15753.82340599 16529.56091252 19506.05756447\n",
      " 18495.21246822 14542.5021054  15339.03756253 14781.65889051\n",
      " 18917.15026522 15382.26955008]\n",
      "Mean: 16548.407477478064\n",
      "standard deviation: 1698.3819733469888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    0.1s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ridge cross_val_score took 0.26195216178894043 seconds\n",
      "Scores: [16560.89418731 17344.85348149 18489.43993088 21188.41763533\n",
      " 21404.17628041 18707.85496801 17117.2491084  16911.16276057\n",
      " 21166.01485479 16126.7960025 ]\n",
      "Mean: 18501.68592096879\n",
      "standard deviation: 1948.7366954658362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    2.4s remaining:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    2.6s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    4.2s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model lasso cross_val_score took 4.47800087928772 seconds\n",
      "Scores: [16689.53857017 17534.13693141 18665.79967919 21212.99524361\n",
      " 21669.1024858  19150.22856014 17179.49191635 17004.85130203\n",
      " 21122.16463788 16242.46366356]\n",
      "Mean: 18647.077299014658\n",
      "standard deviation: 1944.2122861255384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    0.4s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    0.6s remaining:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model krr cross_val_score took 0.9929986000061035 seconds\n",
      "Scores: [18878.59708653 19395.77316772 20542.22202648 21675.84735331\n",
      " 21199.21152988 19379.86559673 18103.81922507 16844.79217853\n",
      " 21766.21012482 18715.90245376]\n",
      "Mean: 19650.224074282025\n",
      "standard deviation: 1537.6472048988028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    0.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "elastic_net= ElasticNet(alpha = 0.2, l1_ratio = 0.9)\n",
    "crossValidateModel(elastic_net, Xt, Y, \"elasticnet\")\n",
    "ridge_reg = Ridge(alpha = 0.1, solver=\"auto\")\n",
    "crossValidateModel(ridge_reg, Xt, Y, \"ridge\")\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "crossValidateModel(lasso_reg, Xt, Y, \"lasso\")\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "crossValidateModel(KRR, Xt, Y, \"krr\")\n",
    "rf = RandomForestRegressor(random_state=1, n_estimators = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  60 | elapsed:  3.0min remaining:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.2min finished\n",
      "c:\\users\\rajesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "c:\\users\\rajesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\rajesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=0.8, gamma=0.01,\n",
       "                                    importance_type='gain', learning_rate=0.05,\n",
       "                                    max_delta_step=0, max_depth=5,\n",
       "                                    min_child_weight=1.8, missing=None,\n",
       "                                    n_estimators=700, n_jobs=1, nthread=-1,\n",
       "                                    objective='reg:linear', random_state=7,\n",
       "                                    refit=True, reg_alpha=0.9, reg_lambda=0.9,\n",
       "                                    scale_pos_weight=1, seed=None, silent=1,\n",
       "                                    subsample=0.5, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'learning_rate': [0.05, 0.2, 0.5, 0.9],\n",
       "                          'max_depth': [3, 5, 7]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try xgboost\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.8, subsample=0.5,\n",
    "                             learning_rate=0.05, max_depth=5, \n",
    "                             min_child_weight=1.8, n_estimators=700,\n",
    "                             reg_alpha=0.9, reg_lambda=0.9, gamma=0.01, \n",
    "                             silent=1, random_state =7, nthread = -1, refit = True)\n",
    "\n",
    "# elastic net seems to give the best scores. use a gridsearchCV to find the best elasticNet parameters\n",
    "param_grid = [\n",
    "    {'learning_rate' : [0.05, 0.2,  0.5,  0.9], 'max_depth': [ 3,  5, 7]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, scoring = \"neg_mean_absolute_error\", n_jobs = -1, verbose = 10, cv = 5)\n",
    "grid_search.fit(Xt, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 3}\n",
      "-14957.826186661217\n",
      "2.724998950958252\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_ )\n",
    "print(grid_search.best_score_ )\n",
    "print(grid_search.refit_time_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8, gamma=0.01,\n",
      "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "             max_depth=3, min_child_weight=1.8, missing=None, n_estimators=700,\n",
      "             n_jobs=1, nthread=-1, objective='reg:linear', random_state=7,\n",
      "             refit=True, reg_alpha=0.9, reg_lambda=0.9, scale_pos_weight=1,\n",
      "             seed=None, silent=1, subsample=0.5, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "xgb_model = grid_search.best_estimator_\n",
    "print(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ElasticNet(alpha=0.2, copy_X=True, fit_intercept=True, l1_ratio=0.9,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False), Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=300,\n",
      "                      n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                      warm_start=False), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8, gamma=0.01,\n",
      "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "             max_depth=3, min_child_weight=1.8, missing=None, n_estimators=300,\n",
      "             n_jobs=1, nthread=-1, objective='reg:linear', random_state=7,\n",
      "             reg_alpha=0.1, reg_lambda=0.3, scale_pos_weight=1, seed=None,\n",
      "             silent=1, subsample=0.5, verbosity=1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 246644542555.24, tolerance: 905296277.1952931\n",
      "  positive)\n",
      "c:\\users\\rajesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "c:\\users\\rajesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AveragingModels(models=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import AveragedModels as av\n",
    "avmodel = av.AveragingModels(models = (elastic_net, lasso_reg, rf, xgb_model))\n",
    "avmodel.fit(Xt,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 80)\n",
      "(1459, 64)\n"
     ]
    }
   ],
   "source": [
    "# use the grid serach results to create the final predictor for the test\n",
    "xgb_model.fit(Xt,Y)\n",
    "\n",
    "#path to file you will use for predictions\n",
    "test_data_path = '../data/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "print(test_data.shape)\n",
    "X_test = test_data.drop(columns = columns_to_ignore)\n",
    "X_test = X_test.drop(columns = missingData.keys())\n",
    "print(X_test.shape)\n",
    "X_pca_test = pca_pipeline.transform(X_test)\n",
    "# categorical colums that are OHE\n",
    "X_cat_test = cat_pipeline.transform(X_test[cat_features_names])\n",
    "\n",
    "X_final_test = np.concatenate((X_pca_test, X_cat_test.todense() ), axis = 1)\n",
    "#make predictions which we will submit. \n",
    "y_pred = xgb_model.predict(X_final_test)\n",
    "\n",
    "#The lines below shows how to save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': y_pred})\n",
    "output.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
